{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Input, Embedding, GRU, Flatten\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import codecs\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle \n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sutras_fp = str(os.getcwd()) + r'\\datasets\\Sutras\\Sutras_full.txt'\n",
    "vedas_fp = str(os.getcwd()) + r'\\datasets\\Vedas\\vedas.txt'\n",
    "quran_fp = str(os.getcwd()) + r'\\datasets\\Quran\\quran.txt'\n",
    "tanakh_fp = str(os.getcwd()) + r'\\datasets\\Tanakh\\Pentateuch_full.txt'\n",
    "bible_fp = str(os.getcwd()) + r'\\datasets\\Bible\\bible.txt'\n",
    "#religious_texts_fp = str(os.getcwd()) + r'\\datasets\\religious_texts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEQUENCE_LEN = 10\n",
    "MIN_WORD_FREQUENCY = 5\n",
    "STEP = 1\n",
    "BATCH_SIZE = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData():\n",
    "    def __init__(self, corpus_file):\n",
    "        self.text = None\n",
    "        self.tokens = None\n",
    "        self.token_freq = None\n",
    "        self.clean_text = None\n",
    "        \"\"\"Takes a corpus filename, opens a utf-8 file, \"\"\"\n",
    "\n",
    "        with io.open(corpus_file, encoding='utf-8') as f:\n",
    "            self.text = f.read().lower().replace('\\n', ' \\n ')\n",
    "            self.text = self.text.replace('\\t', '')\n",
    "\n",
    "            print('Corpus length in characters:', len(self.text))     \n",
    "        self.corpus_len = len(self.text)\n",
    "        print('Loaded corpus...')\n",
    "\n",
    "    \n",
    "    def get_token_freq(self):\n",
    "        # Calculate word frequency\n",
    "        self.token_freq = {}\n",
    "        for token in self.tokens:\n",
    "            self.token_freq[token] =  self.token_freq.get(token, 0) + 1\n",
    "        return self.token_freq\n",
    "    \n",
    "    def get_char_freq(self):\n",
    "        # Calculate word frequency\n",
    "        self.char_freq = {}\n",
    "        for char in self.text:\n",
    "            self.char_freq[char] =  self.char_freq.get(char, 0) + 1\n",
    "        return self.char_freq\n",
    "\n",
    "    \n",
    "    def tokenize(self):\n",
    "        self.tokens = [w for w in self.text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "        print('Corpus length in words:', len(self.tokens))\n",
    "        return self.tokens\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_chars(txt):\n",
    "    #return re.sub('[^A-Za-z0-9.,?!\" -â€“]+', '', txt)\n",
    "    return re.sub('[^A-Za-z0-9 ]+', '', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ignorable_words(word_freq):\n",
    "    ignored_words = set()\n",
    "    for k, v in word_freq.items():\n",
    "        if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "            ignored_words.add(k)\n",
    "    return ignored_words\n",
    "\n",
    "\n",
    "def get_words(text_in_words, ignored_words):\n",
    "    words = set(text_in_words)\n",
    "    print('Unique words before ignoring:', len(words))\n",
    "    print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "    words = sorted(set(words) - ignored_words)\n",
    "    print('Unique words after ignoring:', len(words))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_translate_dicts(words):\n",
    "    word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "    indices_word = dict((i, c) for i, c in enumerate(words))\n",
    "    return word_indices, indices_word\n",
    "\n",
    "\n",
    "def get_training_data(text_in_words, ignored_words):\n",
    "    # cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
    "    sentences = []\n",
    "    next_words = []\n",
    "    ignored = 0\n",
    "    for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n",
    "        # Only add the sequences where no word is in ignored_words\n",
    "        if len(set(text_in_words[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
    "            sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n",
    "            next_words.append(text_in_words[i + SEQUENCE_LEN])\n",
    "        else:\n",
    "            ignored = ignored + 1\n",
    "    print('Ignored sequences:', ignored)\n",
    "    print('Remaining sequences:', len(sentences))\n",
    "    return sentences, next_words\n",
    "\n",
    "\n",
    "def get_training_data_one_list(text_in_words, ignored_words):\n",
    "    # cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
    "    text = []\n",
    "    ignored = 0\n",
    "    for i in text_in_words:\n",
    "        # Only add the sequences where no word is in ignored_words\n",
    "        if i not in ignored_words:\n",
    "            text.append(i)\n",
    "        else:\n",
    "            ignored = ignored + 1\n",
    "    print('Ignored sequences:', ignored)\n",
    "    print('Remaining sequences:', len(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_vocabulary(words_file_path, words_set):\n",
    "    \"\"\"Takes a filepath and a list, and creates a text file to log that word list\"\"\"\n",
    "    words_file = codecs.open(words_file_path, 'w', encoding='utf8')\n",
    "    for w in words_set:\n",
    "        if w != \"\\n\":\n",
    "            words_file.write(w+\"\\n\")\n",
    "        else:\n",
    "            words_file.write(w)\n",
    "    words_file.close()\n",
    "    \n",
    "    \n",
    "def plot_log_char_freq(char_freq, word_freq):\n",
    "    df1 = pd.DataFrame(char_freq, index = [0])\n",
    "    df2 = pd.DataFrame(word_freq, index = [0])\n",
    "    fig, (ax1,ax2) = plt.subplots(nrows = 2, ncols = 1, figsize = (20,8));\n",
    "   \n",
    "    ax1.bar(range(len(df1.columns)),np.log(df1.iloc[0].values), width = 2);\n",
    "    ax1.set_title('Character Frequency in the corpus');\n",
    "    ax1.set_ylabel('Log Scale of Frequency');\n",
    "    ax1.set_xlabel('Character');\n",
    "\n",
    "    ax2.bar(range(len(df2.columns)),np.log(df2.iloc[0].values), width = 2);\n",
    "    ax2.set_title('Word Frequency in the corpus');\n",
    "    ax2.set_ylabel('Log Scale of Frequency');\n",
    "    ax2.set_xlabel('Word');\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in characters: 7915214\n",
      "Loaded corpus...\n",
      "Corpus length in characters: 3808286\n",
      "Loaded corpus...\n",
      "Corpus length in characters: 4552748\n",
      "Loaded corpus...\n",
      "Corpus length in characters: 869286\n",
      "Loaded corpus...\n",
      "Corpus length in characters: 764579\n",
      "Loaded corpus...\n"
     ]
    }
   ],
   "source": [
    "sutras = TextData(sutras_fp)\n",
    "vedas = TextData(vedas_fp)\n",
    "bible = TextData(bible_fp)\n",
    "tanakh = TextData(tanakh_fp)\n",
    "quran = TextData(quran_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sutras\n",
    "\n",
    "1. Set up, preprocess, clean, and plot initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in words: 1318748\n",
      "Unique words before ignoring: 28369\n",
      "Ignoring words with frequency < 5\n",
      "Unique words after ignoring: 10205\n",
      "Encoder Table:\n",
      " [('1', 0), ('10', 1), ('100', 2), ('1000', 3), ('102', 4), ('103', 5), ('104', 6), ('106', 7), ('107', 8), ('108', 9)] \n",
      "Decoder Table:\n",
      " [(0, '1'), (1, '10'), (2, '100'), (3, '1000'), (4, '102'), (5, '103'), (6, '104'), (7, '106'), (8, '107'), (9, '108')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## First drop and replace some bad characters to make the analysis simpler and more effective\n",
    "sutras.text = drop_chars(sutras.text)\n",
    "#Get and count the frequency of each character after cleaning\n",
    "sutras_char_freq = sutras.get_char_freq()\n",
    "\n",
    "#Basically split into words (words are tokens here)\n",
    "sutras_tokens = sutras.tokenize() # tokens\n",
    "#Get and count the freq of each word/token PRE IGNORED WORDS\n",
    "sutras_freq = sutras.get_token_freq() # freq\n",
    "\n",
    "#Get ignorable words less than the global var min word frequency\n",
    "sutras_ignored_words = get_ignorable_words(sutras_freq) #ignored words less than frequency\n",
    "sutras_kept_words = get_words(sutras_tokens, sutras_ignored_words) #keep words\n",
    "\n",
    "\n",
    "#Get translation tables for kept tokens\n",
    "sutras_encoder, sutras_decoder = get_translate_dicts(sutras_kept_words) #get translation tables\n",
    "print(\"Encoder Table:\\n\",list(sutras_encoder.items())[:10],\"\\nDecoder Table:\\n\", list(sutras_decoder.items())[:10])\n",
    "\n",
    "#Get the frequency of each word after pruning\n",
    "sutras_kept_freq = dict(zip(sutras_kept_words,[sutras_freq[value] for value in sutras_kept_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(sutras_kept_freq,'sutras_kept_freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>3296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>which</th>\n",
       "      <td>3493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>him</th>\n",
       "      <td>3538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>3539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>3551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>their</th>\n",
       "      <td>3595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>3644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>these</th>\n",
       "      <td>3654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thus</th>\n",
       "      <td>3678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>should</th>\n",
       "      <td>3770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>3897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>then</th>\n",
       "      <td>4118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>4198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blessed</th>\n",
       "      <td>4369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mind</th>\n",
       "      <td>4401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if</th>\n",
       "      <td>4416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>4687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has</th>\n",
       "      <td>4803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what</th>\n",
       "      <td>4929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>when</th>\n",
       "      <td>4956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will</th>\n",
       "      <td>5185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>5627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>5828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>who</th>\n",
       "      <td>5928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>6259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>there</th>\n",
       "      <td>6845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>6956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>his</th>\n",
       "      <td>7020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>7457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>7778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>or</th>\n",
       "      <td>8182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>8339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>8798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>8976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>9554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>9899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>11590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>11630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>12097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>12395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>12976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>13086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>15620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>22626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>24146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>29482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>30777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>46662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>49595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "so        3296\n",
       "which     3493\n",
       "him       3538\n",
       "an        3539\n",
       "no        3551\n",
       "their     3595\n",
       "said      3644\n",
       "these     3654\n",
       "thus      3678\n",
       "should    3770\n",
       "at        3897\n",
       "then      4118\n",
       "was       4198\n",
       "blessed   4369\n",
       "mind      4401\n",
       "if        4416\n",
       "have      4687\n",
       "has       4803\n",
       "what      4929\n",
       "when      4956\n",
       "will      5185\n",
       "they      5627\n",
       "all       5828\n",
       "who       5928\n",
       "you       6259\n",
       "there     6845\n",
       "on        6956\n",
       "his       7020\n",
       "from      7457\n",
       "by        7778\n",
       "or        8182\n",
       "for       8339\n",
       "be        8798\n",
       "i         8976\n",
       "are       9554\n",
       "as        9899\n",
       "with     11590\n",
       "it       11630\n",
       "one      12097\n",
       "this     12395\n",
       "not      12976\n",
       "he       13086\n",
       "that     15620\n",
       "a        22626\n",
       "in       24146\n",
       "is       29482\n",
       "to       30777\n",
       "and      46662\n",
       "of       49595"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.DataFrame(sutras_kept_freq, index = [0]).T\n",
    "s.sort_values(by= [0])[-50:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot_log_char_freq(sutras_char_freq, sutras_kept_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vedas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in words: 629129\n",
      "Unique words before ignoring: 20324\n",
      "Ignoring words with frequency < 5\n",
      "Unique words after ignoring: 6016\n",
      "Encoder Table:\n",
      " [('0', 0), ('1', 1), ('10', 2), ('1009', 3), ('1010', 4), ('1021', 5), ('1022', 6), ('1023', 7), ('1035', 8), ('1036', 9)] \n",
      "Decoder Table:\n",
      " [(0, '0'), (1, '1'), (2, '10'), (3, '1009'), (4, '1010'), (5, '1021'), (6, '1022'), (7, '1023'), (8, '1035'), (9, '1036')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## First drop and replace some bad characters to make the analysis simpler and more effective\n",
    "vedas.text = drop_chars(vedas.text)\n",
    "#Get and count the frequency of each character after cleaning\n",
    "vedas_char_freq = vedas.get_char_freq()\n",
    "\n",
    "#Basically split into words (words are tokens here)\n",
    "vedas_tokens = vedas.tokenize() # tokens\n",
    "#Get and count the freq of each word/token PRE IGNORED WORDS\n",
    "vedas_freq = vedas.get_token_freq() # freq\n",
    "\n",
    "#Get ignorable words less than the global var min word frequency\n",
    "vedas_ignored_words = get_ignorable_words(vedas_freq) #ignored words less than frequency\n",
    "vedas_kept_words = get_words(vedas_tokens, vedas_ignored_words) #keep words\n",
    "\n",
    "\n",
    "#Get translation tables for kept tokens\n",
    "vedas_encoder, vedas_decoder = get_translate_dicts(vedas_kept_words) #get translation tables\n",
    "print(\"Encoder Table:\\n\",list(vedas_encoder.items())[:10],\"\\nDecoder Table:\\n\", list(vedas_decoder.items())[:10])\n",
    "\n",
    "#Get the frequency of each word after pruning\n",
    "vedas_kept_freq = dict(zip(vedas_kept_words,[vedas_freq[value] for value in vedas_kept_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(vedas_kept_freq,'vedas_kept_freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_log_char_freq(vedas_char_freq, vedas_kept_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hath</th>\n",
       "      <td>1814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>1842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>their</th>\n",
       "      <td>1872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>2149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sacrifice</th>\n",
       "      <td>2182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>2372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hymn</th>\n",
       "      <td>2434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verily</th>\n",
       "      <td>2554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soma</th>\n",
       "      <td>2558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ye</th>\n",
       "      <td>2611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>2665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>2850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>2893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>our</th>\n",
       "      <td>2935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>2956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gods</th>\n",
       "      <td>3076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>3123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>3123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>3169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>3193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>3244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thy</th>\n",
       "      <td>3260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>3391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>3395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>his</th>\n",
       "      <td>3746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>may</th>\n",
       "      <td>3761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agni</th>\n",
       "      <td>3937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>3954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thee</th>\n",
       "      <td>4082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>him</th>\n",
       "      <td>4289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indra</th>\n",
       "      <td>4494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>4688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>4951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o</th>\n",
       "      <td>4970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>who</th>\n",
       "      <td>5009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thou</th>\n",
       "      <td>5630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>5923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>6595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>6863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>9356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>10337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>11540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>13535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>13697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>17584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "hath        1814\n",
       "like        1842\n",
       "their       1872\n",
       "one         2009\n",
       "3           2129\n",
       "not         2149\n",
       "sacrifice   2182\n",
       "2           2331\n",
       "have        2372\n",
       "hymn        2434\n",
       "verily      2554\n",
       "soma        2558\n",
       "ye          2611\n",
       "we          2665\n",
       "it          2850\n",
       "i           2893\n",
       "our         2935\n",
       "are         2956\n",
       "gods        3076\n",
       "all         3123\n",
       "by          3123\n",
       "be          3169\n",
       "they        3193\n",
       "on          3244\n",
       "thy         3260\n",
       "this        3391\n",
       "from        3395\n",
       "1           3676\n",
       "his         3746\n",
       "may         3761\n",
       "agni        3937\n",
       "as          3954\n",
       "thee        4082\n",
       "him         4289\n",
       "indra       4494\n",
       "us          4688\n",
       "that        4951\n",
       "o           4970\n",
       "who         5009\n",
       "thou        5630\n",
       "a           5923\n",
       "is          6595\n",
       "for         6863\n",
       "in          9356\n",
       "he         10337\n",
       "with       11540\n",
       "and        13535\n",
       "to         13697\n",
       "of         17584"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.DataFrame(vedas_kept_freq, index = [0]).T\n",
    "s.sort_values(by= [0])[-50:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in words: 824172\n",
      "Unique words before ignoring: 16948\n",
      "Ignoring words with frequency < 5\n",
      "Unique words after ignoring: 6829\n",
      "Encoder Table:\n",
      " [('101', 0), ('1010', 1), ('1011', 2), ('1012', 3), ('1013', 4), ('1014', 5), ('1015', 6), ('1016', 7), ('1017', 8), ('1018', 9)] \n",
      "Decoder Table:\n",
      " [(0, '101'), (1, '1010'), (2, '1011'), (3, '1012'), (4, '1013'), (5, '1014'), (6, '1015'), (7, '1016'), (8, '1017'), (9, '1018')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## First drop and replace some bad characters to make the analysis simpler and more effective\n",
    "bible.text = drop_chars(bible.text)\n",
    "#Get and count the frequency of each character after cleaning\n",
    "bible_char_freq = bible.get_char_freq()\n",
    "\n",
    "#Basically split into words (words are tokens here)\n",
    "bible_tokens = bible.tokenize() # tokens\n",
    "#Get and count the freq of each word/token PRE IGNORED WORDS\n",
    "bible_freq = bible.get_token_freq() # freq\n",
    "\n",
    "#Get ignorable words less than the global var min word frequency\n",
    "bible_ignored_words = get_ignorable_words(bible_freq) #ignored words less than frequency\n",
    "bible_kept_words = get_words(bible_tokens, bible_ignored_words) #keep words\n",
    "\n",
    "\n",
    "#Get translation tables for kept tokens\n",
    "bible_encoder, bible_decoder = get_translate_dicts(bible_kept_words) #get translation tables\n",
    "print(\"Encoder Table:\\n\",list(bible_encoder.items())[:10],\"\\nDecoder Table:\\n\", list(bible_decoder.items())[:10])\n",
    "\n",
    "#Get the frequency of each word after pruning\n",
    "bible_kept_freq = dict(zip(bible_kept_words,[bible_freq[value] for value in bible_kept_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(bible_kept_freq,'bible_kept_freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_char_freq(bible_char_freq, bible_kept_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>up</th>\n",
       "      <td>2383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>israel</th>\n",
       "      <td>2565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>2613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>2644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>2683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upon</th>\n",
       "      <td>2750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>were</th>\n",
       "      <td>2773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out</th>\n",
       "      <td>2776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>2835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>when</th>\n",
       "      <td>2836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>3530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>3657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thee</th>\n",
       "      <td>3826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will</th>\n",
       "      <td>3843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>3909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>their</th>\n",
       "      <td>3932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ye</th>\n",
       "      <td>3983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>3997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>3999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>4368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>which</th>\n",
       "      <td>4421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>god</th>\n",
       "      <td>4442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>4525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thy</th>\n",
       "      <td>4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thou</th>\n",
       "      <td>5474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>5638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>6059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>6144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>them</th>\n",
       "      <td>6430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>6618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>him</th>\n",
       "      <td>6659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>7016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>7032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>7379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lord</th>\n",
       "      <td>7830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>8235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>his</th>\n",
       "      <td>8473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>8854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unto</th>\n",
       "      <td>8997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>8997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shall</th>\n",
       "      <td>9840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>10421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>12725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>12927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>13660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>34791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>51765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "up       2383\n",
       "israel   2565\n",
       "man      2613\n",
       "by       2644\n",
       "you      2683\n",
       "upon     2750\n",
       "were     2773\n",
       "out      2776\n",
       "this     2835\n",
       "when     2836\n",
       "are      2970\n",
       "as       3530\n",
       "from     3657\n",
       "thee     3826\n",
       "will     3843\n",
       "have     3909\n",
       "their    3932\n",
       "ye       3983\n",
       "but      3997\n",
       "said     3999\n",
       "me       4096\n",
       "my       4368\n",
       "which    4421\n",
       "god      4442\n",
       "was      4525\n",
       "thy      4600\n",
       "thou     5474\n",
       "all      5638\n",
       "with     6059\n",
       "it       6144\n",
       "them     6430\n",
       "not      6618\n",
       "him      6659\n",
       "is       7016\n",
       "be       7032\n",
       "they     7379\n",
       "lord     7830\n",
       "a        8235\n",
       "his      8473\n",
       "i        8854\n",
       "unto     8997\n",
       "for      8997\n",
       "shall    9840\n",
       "he      10421\n",
       "in      12725\n",
       "that    12927\n",
       "to      13660\n",
       "of      34791\n",
       "and     51765"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.DataFrame(bible_kept_freq, index = [0]).T\n",
    "s.sort_values(by= [0])[-50:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanakh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in words: 162580\n",
      "Unique words before ignoring: 6789\n",
      "Ignoring words with frequency < 5\n",
      "Unique words after ignoring: 2397\n",
      "Encoder Table:\n",
      " [('101', 0), ('1010', 1), ('1011', 2), ('1012', 3), ('1013', 4), ('1014', 5), ('1015', 6), ('1016', 7), ('1017', 8), ('1018', 9)] \n",
      "Decoder Table:\n",
      " [(0, '101'), (1, '1010'), (2, '1011'), (3, '1012'), (4, '1013'), (5, '1014'), (6, '1015'), (7, '1016'), (8, '1017'), (9, '1018')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## First drop and replace some bad characters to make the analysis simpler and more effective\n",
    "tanakh.text = drop_chars(tanakh.text)\n",
    "#Get and count the frequency of each character after cleaning\n",
    "tanakh_char_freq = tanakh.get_char_freq()\n",
    "\n",
    "#Basically split into words (words are tokens here)\n",
    "tanakh_tokens = tanakh.tokenize() # tokens\n",
    "#Get and count the freq of each word/token PRE IGNORED WORDS\n",
    "tanakh_freq = tanakh.get_token_freq() # freq\n",
    "\n",
    "#Get ignorable words less than the global var min word frequency\n",
    "tanakh_ignored_words = get_ignorable_words(tanakh_freq) #ignored words less than frequency\n",
    "tanakh_kept_words = get_words(tanakh_tokens, tanakh_ignored_words) #keep words\n",
    "\n",
    "\n",
    "#Get translation tables for kept tokens\n",
    "tanakh_encoder, tanakh_decoder = get_translate_dicts(tanakh_kept_words) #get translation tables\n",
    "print(\"Encoder Table:\\n\",list(tanakh_encoder.items())[:10],\"\\nDecoder Table:\\n\", list(tanakh_decoder.items())[:10])\n",
    "\n",
    "#Get the frequency of each word after pruning\n",
    "tanakh_kept_freq = dict(zip(tanakh_kept_words,[tanakh_freq[value] for value in tanakh_kept_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(tanakh_kept_freq,'tanakh_kept_freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_char_freq(tanakh_char_freq, tanakh_kept_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>were</th>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offering</th>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>israel</th>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out</th>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>children</th>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will</th>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moses</th>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>land</th>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upon</th>\n",
       "      <td>758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>god</th>\n",
       "      <td>798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>their</th>\n",
       "      <td>837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ye</th>\n",
       "      <td>844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shalt</th>\n",
       "      <td>893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thee</th>\n",
       "      <td>959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>which</th>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>him</th>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>1092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>1123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>1169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>them</th>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thy</th>\n",
       "      <td>1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>1263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thou</th>\n",
       "      <td>1524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>1637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>1677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lord</th>\n",
       "      <td>1854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>his</th>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>2037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>2220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>2241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>2314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unto</th>\n",
       "      <td>2356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shall</th>\n",
       "      <td>2828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>7282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>11671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "were        504\n",
       "as          524\n",
       "have        528\n",
       "your        538\n",
       "offering    546\n",
       "me          549\n",
       "this        550\n",
       "my          565\n",
       "you         575\n",
       "israel      576\n",
       "out         582\n",
       "children    592\n",
       "will        615\n",
       "moses       648\n",
       "land        683\n",
       "was         690\n",
       "from        696\n",
       "upon        758\n",
       "god         798\n",
       "their       837\n",
       "ye          844\n",
       "said        872\n",
       "shalt       893\n",
       "thee        959\n",
       "which       981\n",
       "him        1052\n",
       "is         1066\n",
       "not        1080\n",
       "they       1092\n",
       "with       1123\n",
       "all        1169\n",
       "them       1225\n",
       "thy        1253\n",
       "i          1263\n",
       "thou       1524\n",
       "it         1637\n",
       "be         1677\n",
       "for        1686\n",
       "a          1768\n",
       "lord       1854\n",
       "his        1984\n",
       "he         2037\n",
       "to         2220\n",
       "that       2241\n",
       "in         2314\n",
       "unto       2356\n",
       "shall      2828\n",
       "of         7282\n",
       "and       11671"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.DataFrame(tanakh_kept_freq, index = [0]).T\n",
    "s.sort_values(by= [0])[-50:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in words: 139057\n",
      "Unique words before ignoring: 6962\n",
      "Ignoring words with frequency < 5\n",
      "Unique words after ignoring: 1981\n",
      "Encoder Table:\n",
      " [('a', 0), ('aad', 1), ('aaron', 2), ('abandon', 3), ('abide', 4), ('abiding', 5), ('able', 6), ('abode', 7), ('about', 8), ('above', 9)] \n",
      "Decoder Table:\n",
      " [(0, 'a'), (1, 'aad'), (2, 'aaron'), (3, 'abandon'), (4, 'abide'), (5, 'abiding'), (6, 'able'), (7, 'abode'), (8, 'about'), (9, 'above')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## First drop and replace some bad characters to make the analysis simpler and more effective\n",
    "quran.text = drop_chars(quran.text)\n",
    "#Get and count the frequency of each character after cleaning\n",
    "quran_char_freq = quran.get_char_freq()\n",
    "\n",
    "#Basically split into words (words are tokens here)\n",
    "quran_tokens = quran.tokenize() # tokens\n",
    "#Get and count the freq of each word/token PRE IGNORED WORDS\n",
    "quran_freq = quran.get_token_freq() # freq\n",
    "\n",
    "#Get ignorable words less than the global var min word frequency\n",
    "quran_ignored_words = get_ignorable_words(quran_freq) #ignored words less than frequency\n",
    "quran_kept_words = get_words(quran_tokens, quran_ignored_words) #keep words\n",
    "\n",
    "\n",
    "#Get translation tables for kept tokens\n",
    "quran_encoder, quran_decoder = get_translate_dicts(quran_kept_words) #get translation tables\n",
    "print(\"Encoder Table:\\n\",list(quran_encoder.items())[:10],\"\\nDecoder Table:\\n\", list(quran_decoder.items())[:10])\n",
    "\n",
    "#Get the frequency of each word after pruning\n",
    "quran_kept_freq = dict(zip(quran_kept_words,[quran_freq[value] for value in quran_kept_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(quran_kept_freq,'quran_kept_freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_log_char_freq(quran_char_freq, quran_kept_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>our</th>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>or</th>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>then</th>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has</th>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if</th>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>when</th>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>his</th>\n",
       "      <td>886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lord</th>\n",
       "      <td>907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>him</th>\n",
       "      <td>1037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>1077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>1092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what</th>\n",
       "      <td>1096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>1123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>those</th>\n",
       "      <td>1164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>1172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>1197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>their</th>\n",
       "      <td>1229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>1416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>1667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>1670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>who</th>\n",
       "      <td>1723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>1724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>1773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>1921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will</th>\n",
       "      <td>2133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>2178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>them</th>\n",
       "      <td>2187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>god</th>\n",
       "      <td>2498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>2770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>2800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>3249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>3501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>4427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>6853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "our      445\n",
       "us       446\n",
       "day      463\n",
       "my       479\n",
       "or       480\n",
       "no       504\n",
       "as       534\n",
       "then     554\n",
       "has      580\n",
       "i        650\n",
       "people   672\n",
       "on       682\n",
       "if       684\n",
       "so       731\n",
       "say      751\n",
       "said     774\n",
       "when     788\n",
       "his      886\n",
       "lord     907\n",
       "be       974\n",
       "him     1037\n",
       "but     1075\n",
       "with    1077\n",
       "do      1092\n",
       "what    1096\n",
       "from    1123\n",
       "your    1128\n",
       "those   1164\n",
       "that    1172\n",
       "have    1197\n",
       "their   1229\n",
       "are     1416\n",
       "it      1667\n",
       "not     1670\n",
       "who     1723\n",
       "for     1724\n",
       "we      1773\n",
       "in      1921\n",
       "a       1967\n",
       "will    2133\n",
       "he      2178\n",
       "them    2187\n",
       "god     2498\n",
       "is      2770\n",
       "they    2800\n",
       "to      3249\n",
       "of      3501\n",
       "you     4427\n",
       "and     6853"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.DataFrame(quran_kept_freq, index = [0]).T\n",
    "s.sort_values(by= [0])[-50:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sutras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored sequences: 236873\n",
      "Remaining sequences: 1081865\n"
     ]
    }
   ],
   "source": [
    "# sutras_sentences, sutras_next_words = get_training_data(sutras_tokens, sutras_ignored_words)\n",
    "\n",
    "# #Set up the full encoded training list\n",
    "# sutras_encoded = []\n",
    "\n",
    "# #Update the list with the encoded values\n",
    "# for seq in sutras_sentences:\n",
    "#     int_sutras_encode = []\n",
    "#     for v in seq:\n",
    "#         int_sutras_encode.append(sutras_encoder[v])\n",
    "#     sutras_encoded.append(int_sutras_encode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored sequences: 30180\n",
      "Remaining sequences: 1288568\n",
      "Original Text in Words Length:  1318748\n",
      "After Ignored Words Text in Words Length:  1288568\n"
     ]
    }
   ],
   "source": [
    "sutras_t = get_training_data_one_list(sutras_tokens, sutras_ignored_words)\n",
    "print('Original Text in Words Length: ',len(sutras_tokens))\n",
    "print('After Ignored Words Text in Words Length: ',len(sutras_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'its',\n",
       " 'consummation',\n",
       " 'doth',\n",
       " 'he',\n",
       " 'proclaim',\n",
       " 'both',\n",
       " 'in',\n",
       " 'the',\n",
       " 'spirit',\n",
       " 'and',\n",
       " 'in',\n",
       " 'the',\n",
       " 'letter',\n",
       " 'the',\n",
       " 'higher',\n",
       " 'life',\n",
       " 'doth',\n",
       " 'he',\n",
       " 'make',\n",
       " 'known',\n",
       " 'in',\n",
       " 'all',\n",
       " 'its',\n",
       " 'fullness',\n",
       " 'and',\n",
       " 'in',\n",
       " 'all',\n",
       " 'its',\n",
       " 'purity',\n",
       " 'and',\n",
       " 'good',\n",
       " 'is',\n",
       " 'it',\n",
       " 'to',\n",
       " 'pay',\n",
       " 'visits',\n",
       " 'to',\n",
       " 'arahats',\n",
       " 'like',\n",
       " 'that',\n",
       " '3',\n",
       " 'now',\n",
       " 'at',\n",
       " 'that',\n",
       " 'time',\n",
       " 'a',\n",
       " 'young',\n",
       " 'brahman',\n",
       " 'an']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sutras_t[50000:50050]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vedas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vedas_sentences, vedas_next_words = get_training_data(vedas_tokens, vedas_ignored_words)\n",
    "\n",
    "# #Set up the full encoded training list\n",
    "# vedas_encoded = []\n",
    "\n",
    "# #Update the list with the encoded values\n",
    "# for seq in vedas_sentences:\n",
    "#     int_vedas_encode = []\n",
    "#     for v in seq:\n",
    "#         int_vedas_encode.append(vedas_encoder[v])\n",
    "#     vedas_encoded.append(int_vedas_encode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored sequences: 23711\n",
      "Remaining sequences: 605418\n",
      "Original Text in Words Length:  629129\n",
      "After Ignored Words Text in Words Length:  605418\n"
     ]
    }
   ],
   "source": [
    "vedas_t = get_training_data_one_list(vedas_tokens, vedas_ignored_words)\n",
    "print('Original Text in Words Length: ',len(vedas_tokens))\n",
    "print('After Ignored Words Text in Words Length: ',len(vedas_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bible_sentences, bible_next_words = get_training_data(bible_tokens, bible_ignored_words)\n",
    "\n",
    "# #Set up the full encoded training list\n",
    "# bible_encoded = []\n",
    "\n",
    "# #Update the list with the encoded values\n",
    "# for seq in bible_sentences:\n",
    "#     int_bible_encode = []\n",
    "#     for v in seq:\n",
    "#         int_bible_encode.append(bible_encoder[v])\n",
    "#     bible_encoded.append(int_bible_encode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored sequences: 17366\n",
      "Remaining sequences: 806806\n",
      "Original Text in Words Length:  824172\n",
      "After Ignored Words Text in Words Length:  806806\n"
     ]
    }
   ],
   "source": [
    "bible_t = get_training_data_one_list(bible_tokens, bible_ignored_words)\n",
    "print('Original Text in Words Length: ',len(bible_tokens))\n",
    "print('After Ignored Words Text in Words Length: ',len(bible_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanakh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanakh_sentences, tanakh_next_words = get_training_data(tanakh_tokens, tanakh_ignored_words)\n",
    "\n",
    "# #Set up the full encoded training list\n",
    "# tanakh_encoded = []\n",
    "\n",
    "# #Update the list with the encoded values\n",
    "# for seq in tanakh_sentences:\n",
    "#     int_tanakh_encode = []\n",
    "#     for v in seq:\n",
    "#         int_tanakh_encode.append(tanakh_encoder[v])\n",
    "#     tanakh_encoded.append(int_tanakh_encode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored sequences: 8047\n",
      "Remaining sequences: 154533\n",
      "Original Text in Words Length:  162580\n",
      "After Ignored Words Text in Words Length:  154533\n"
     ]
    }
   ],
   "source": [
    "tanakh_t = get_training_data_one_list(tanakh_tokens, tanakh_ignored_words)\n",
    "print('Original Text in Words Length: ',len(tanakh_tokens))\n",
    "print('After Ignored Words Text in Words Length: ',len(tanakh_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quran_sentences, quran_next_words = get_training_data(quran_tokens, quran_ignored_words)\n",
    "\n",
    "# #Set up the full encoded training list\n",
    "# quran_encoded = []\n",
    "\n",
    "# #Update the list with the encoded values\n",
    "# for seq in quran_sentences:\n",
    "#     int_quran_encode = []\n",
    "#     for v in seq:\n",
    "#         int_quran_encode.append(quran_encoder[v])\n",
    "#     quran_encoded.append(int_quran_encode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored sequences: 8380\n",
      "Remaining sequences: 130677\n",
      "Original Text in Words Length:  139057\n",
      "After Ignored Words Text in Words Length:  130677\n"
     ]
    }
   ],
   "source": [
    "quran_t = get_training_data_one_list(quran_tokens, quran_ignored_words)\n",
    "print('Original Text in Words Length: ',len(quran_tokens))\n",
    "print('After Ignored Words Text in Words Length: ',len(quran_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sutras_encoded_t = [sutras_encoder[i] for i in sutras_t]\n",
    "vedas_encoded_t = [vedas_encoder[i] for i in vedas_t]\n",
    "bible_encoded_t = [bible_encoder[i] for i in bible_t]\n",
    "tanakh_encoded_t = [tanakh_encoder[i] for i in tanakh_t]\n",
    "quran_encoded_t = [quran_encoder[i] for i in quran_t]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Concatenate\n",
    "from tensorflow.keras.layers import Embedding, dot, Dot\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "import urllib\n",
    "import collections\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {'sutras':[],\n",
    "         'vedas':[],\n",
    "         'bible':[],\n",
    "         'tanakh':[],\n",
    "         'quran':[]}\n",
    "\n",
    "\n",
    "window_size = 6\n",
    "vector_dim = 150\n",
    "epochs = 250000\n",
    "\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sutras Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9087, 2941], [8118, 2117], [2281, 258], [9862, 9190], [8210, 2426], [1301, 5743], [6144, 7062], [4190, 8997], [4915, 4524], [9040, 9087]] [0, 0, 1, 0, 0, 0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#The size of each vocabulary of each text\n",
    "vocab_size = len(sutras_encoder)\n",
    "\n",
    "\n",
    "\n",
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = skipgrams(sutras_encoded_t, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 796,  569,  652, ...,  256,  200, 1205])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss=0.24973024427890778\n",
      "Iteration 1000, loss=0.25718575716018677\n",
      "Iteration 2000, loss=0.25558632612228394\n",
      "Iteration 3000, loss=0.2433577924966812\n",
      "Iteration 4000, loss=0.260622501373291\n",
      "Iteration 5000, loss=0.25719791650772095\n",
      "Iteration 6000, loss=0.23097988963127136\n",
      "Iteration 7000, loss=0.20916429162025452\n",
      "Iteration 8000, loss=0.3073486089706421\n",
      "Iteration 9000, loss=0.19192072749137878\n",
      "Iteration 10000, loss=0.19433946907520294\n",
      "Iteration 11000, loss=0.18392619490623474\n",
      "Iteration 12000, loss=6.10245388088515e-06\n",
      "Iteration 13000, loss=0.07408968359231949\n",
      "Iteration 14000, loss=0.15859222412109375\n",
      "Iteration 15000, loss=0.0\n",
      "Iteration 16000, loss=0.14458470046520233\n",
      "Iteration 17000, loss=0.16416716575622559\n",
      "Iteration 18000, loss=0.0\n",
      "Iteration 19000, loss=0.14185231924057007\n",
      "Iteration 20000, loss=0.40220674872398376\n",
      "Iteration 21000, loss=1.080570655176416e-08\n",
      "Iteration 22000, loss=0.12044154107570648\n",
      "Iteration 23000, loss=0.12080275267362595\n",
      "Iteration 24000, loss=0.4184947907924652\n",
      "Iteration 25000, loss=0.1622585654258728\n",
      "Iteration 26000, loss=0.11619251221418381\n",
      "Iteration 27000, loss=0.12574687600135803\n",
      "Iteration 28000, loss=0.5550995469093323\n",
      "Iteration 29000, loss=0.35841104388237\n",
      "Iteration 30000, loss=0.037461139261722565\n",
      "Iteration 31000, loss=0.09715898334980011\n",
      "Iteration 32000, loss=0.01581178605556488\n",
      "Iteration 33000, loss=8.881784197001252e-12\n",
      "Iteration 34000, loss=0.3180404007434845\n",
      "Iteration 35000, loss=6.87805368215777e-10\n",
      "Iteration 36000, loss=0.006610352545976639\n",
      "Iteration 37000, loss=0.0\n",
      "Iteration 38000, loss=0.10130457580089569\n",
      "Iteration 39000, loss=8.100542459033022e-09\n",
      "Iteration 40000, loss=0.11527970433235168\n",
      "Iteration 41000, loss=0.39621657133102417\n",
      "Iteration 42000, loss=0.15367254614830017\n",
      "Iteration 43000, loss=0.05906544253230095\n",
      "Iteration 44000, loss=0.0\n",
      "Iteration 45000, loss=0.1802133470773697\n",
      "Iteration 46000, loss=0.8652156591415405\n",
      "Iteration 47000, loss=0.0\n",
      "Iteration 48000, loss=0.07311829924583435\n",
      "Iteration 49000, loss=0.4738459289073944\n",
      "Iteration 50000, loss=0.8998289704322815\n",
      "Iteration 51000, loss=0.07531290501356125\n",
      "Iteration 52000, loss=0.17321771383285522\n",
      "Iteration 53000, loss=0.017206234857439995\n",
      "Iteration 54000, loss=0.3862016797065735\n",
      "Iteration 55000, loss=0.081679567694664\n",
      "Iteration 56000, loss=0.034255221486091614\n",
      "Iteration 57000, loss=0.03946111723780632\n",
      "Iteration 58000, loss=0.027867183089256287\n",
      "Iteration 59000, loss=0.6362723112106323\n",
      "Iteration 60000, loss=0.014194083400070667\n",
      "Iteration 61000, loss=0.0013977487105876207\n",
      "Iteration 62000, loss=0.45260852575302124\n",
      "Iteration 63000, loss=0.01728617027401924\n",
      "Iteration 64000, loss=1.0\n",
      "Iteration 65000, loss=1.2054229614477663e-08\n",
      "Iteration 66000, loss=0.06739740073680878\n",
      "Iteration 67000, loss=0.05135498568415642\n",
      "Iteration 68000, loss=0.0\n",
      "Iteration 69000, loss=0.3541715145111084\n",
      "Iteration 70000, loss=6.052214871488104e-07\n",
      "Iteration 71000, loss=0.0002941424318123609\n",
      "Iteration 72000, loss=1.727352719171904e-05\n",
      "Iteration 73000, loss=0.0748562291264534\n",
      "Iteration 74000, loss=0.21678459644317627\n",
      "Iteration 75000, loss=0.3979043662548065\n",
      "Iteration 76000, loss=0.0\n",
      "Iteration 77000, loss=0.01871674507856369\n",
      "Iteration 78000, loss=0.04822547733783722\n",
      "Iteration 79000, loss=0.05538320541381836\n",
      "Iteration 80000, loss=0.0024130402598530054\n",
      "Iteration 81000, loss=6.87805368215777e-12\n",
      "Iteration 82000, loss=0.003936116583645344\n",
      "Iteration 83000, loss=0.8376281261444092\n",
      "Iteration 84000, loss=0.5479239821434021\n",
      "Iteration 85000, loss=0.9238711595535278\n",
      "Iteration 86000, loss=0.0\n",
      "Iteration 87000, loss=0.003793171839788556\n",
      "Iteration 88000, loss=0.4661555290222168\n",
      "Iteration 89000, loss=0.9632187485694885\n",
      "Iteration 90000, loss=0.46534350514411926\n",
      "Iteration 91000, loss=0.0697835311293602\n",
      "Iteration 92000, loss=0.5853704214096069\n",
      "Iteration 93000, loss=3.958008107929345e-07\n",
      "Iteration 94000, loss=0.05657279118895531\n",
      "Iteration 95000, loss=0.6460508108139038\n",
      "Iteration 96000, loss=0.06413009762763977\n",
      "Iteration 97000, loss=0.3198619484901428\n",
      "Iteration 98000, loss=0.000690442742779851\n",
      "Iteration 99000, loss=0.00015875513781793416\n",
      "Iteration 100000, loss=0.544693648815155\n",
      "Iteration 101000, loss=3.0375564165296964e-05\n",
      "Iteration 102000, loss=0.03650357201695442\n",
      "Iteration 103000, loss=0.004440499935299158\n",
      "Iteration 104000, loss=0.6886754035949707\n",
      "Iteration 105000, loss=0.005987442098557949\n",
      "Iteration 106000, loss=0.0\n",
      "Iteration 107000, loss=0.0\n",
      "Iteration 108000, loss=2.6487718969292473e-06\n",
      "Iteration 109000, loss=0.00021915858087595552\n",
      "Iteration 110000, loss=0.02995545230805874\n",
      "Iteration 111000, loss=0.043031856417655945\n",
      "Iteration 112000, loss=0.5765392780303955\n",
      "Iteration 113000, loss=0.0504111647605896\n",
      "Iteration 114000, loss=0.0\n",
      "Iteration 115000, loss=0.6922189593315125\n",
      "Iteration 116000, loss=0.010550014674663544\n",
      "Iteration 117000, loss=1.1951328815484885e-11\n",
      "Iteration 118000, loss=0.06551288813352585\n",
      "Iteration 119000, loss=0.0523541234433651\n",
      "Iteration 120000, loss=0.0\n",
      "Iteration 121000, loss=0.7387518286705017\n",
      "Iteration 122000, loss=4.921131676383084e-06\n",
      "Iteration 123000, loss=0.06463424116373062\n",
      "Iteration 124000, loss=0.04844653606414795\n",
      "Iteration 125000, loss=0.018876483663916588\n",
      "Iteration 126000, loss=0.0\n",
      "Iteration 127000, loss=0.01708044297993183\n",
      "Iteration 128000, loss=0.027186905965209007\n",
      "Iteration 129000, loss=5.609555955743417e-06\n",
      "Iteration 130000, loss=0.03135916590690613\n",
      "Iteration 131000, loss=0.0009219347848556936\n",
      "Iteration 132000, loss=0.0\n",
      "Iteration 133000, loss=0.0\n",
      "Iteration 134000, loss=0.03845598176121712\n",
      "Iteration 135000, loss=0.0051117390394210815\n",
      "Iteration 136000, loss=0.7790315747261047\n",
      "Iteration 137000, loss=0.8883661031723022\n",
      "Iteration 138000, loss=0.09612724184989929\n",
      "Iteration 139000, loss=0.0\n",
      "Iteration 140000, loss=0.07824288308620453\n",
      "Iteration 141000, loss=6.40117036709853e-07\n",
      "Iteration 142000, loss=0.07494430243968964\n",
      "Iteration 143000, loss=0.0006076015415601432\n",
      "Iteration 144000, loss=1.5076295767357806e-10\n",
      "Iteration 145000, loss=0.057480588555336\n",
      "Iteration 146000, loss=6.549415411427617e-05\n",
      "Iteration 147000, loss=0.0\n",
      "Iteration 148000, loss=0.011073734611272812\n",
      "Iteration 149000, loss=0.04819644242525101\n",
      "Iteration 150000, loss=0.005200262181460857\n",
      "Iteration 151000, loss=0.8654420971870422\n",
      "Iteration 152000, loss=0.5244468450546265\n",
      "Iteration 153000, loss=0.1042083129286766\n",
      "Iteration 154000, loss=0.6589615941047668\n",
      "Iteration 155000, loss=0.06086339056491852\n",
      "Iteration 156000, loss=1.0027179087046534e-10\n",
      "Iteration 157000, loss=0.023727308958768845\n",
      "Iteration 158000, loss=0.036215998232364655\n",
      "Iteration 159000, loss=0.4501707851886749\n",
      "Iteration 160000, loss=1.1844419134376949e-07\n",
      "Iteration 161000, loss=0.9190422892570496\n",
      "Iteration 162000, loss=3.16796359811633e-07\n",
      "Iteration 163000, loss=0.174782395362854\n",
      "Iteration 164000, loss=0.0\n",
      "Iteration 165000, loss=2.049456043096143e-06\n",
      "Iteration 166000, loss=0.0012873602099716663\n",
      "Iteration 167000, loss=0.0506853386759758\n",
      "Iteration 168000, loss=0.18794894218444824\n",
      "Iteration 169000, loss=0.042610589414834976\n",
      "Iteration 170000, loss=0.9696603417396545\n",
      "Iteration 171000, loss=0.06573965400457382\n",
      "Iteration 172000, loss=0.0\n",
      "Iteration 173000, loss=0.004598905332386494\n",
      "Iteration 174000, loss=0.0\n",
      "Iteration 175000, loss=0.2652890980243683\n",
      "Iteration 176000, loss=7.17188015642023e-08\n",
      "Iteration 177000, loss=0.0\n",
      "Iteration 178000, loss=0.31453511118888855\n",
      "Iteration 179000, loss=5.115907697472721e-13\n",
      "Iteration 180000, loss=0.014478757977485657\n",
      "Iteration 181000, loss=1.4959425698179984e-08\n",
      "Iteration 182000, loss=0.00027983609470538795\n",
      "Iteration 183000, loss=5.280580080579966e-05\n",
      "Iteration 184000, loss=0.5010387897491455\n",
      "Iteration 185000, loss=0.010416803881525993\n",
      "Iteration 186000, loss=0.08294756710529327\n",
      "Iteration 187000, loss=0.051229942589998245\n",
      "Iteration 188000, loss=0.07244323939085007\n",
      "Iteration 189000, loss=0.00048577532288618386\n",
      "Iteration 190000, loss=0.19799655675888062\n",
      "Iteration 191000, loss=2.6521604013396427e-05\n",
      "Iteration 192000, loss=0.6287352442741394\n",
      "Iteration 193000, loss=0.16346803307533264\n",
      "Iteration 194000, loss=0.07386720925569534\n",
      "Iteration 195000, loss=0.0038785662036389112\n",
      "Iteration 196000, loss=0.003976977895945311\n",
      "Iteration 197000, loss=0.9803812503814697\n",
      "Iteration 198000, loss=3.212402225472033e-05\n",
      "Iteration 199000, loss=2.929084530478576e-09\n",
      "Iteration 200000, loss=0.0\n",
      "Iteration 201000, loss=0.9262284636497498\n",
      "Iteration 202000, loss=0.0164601169526577\n",
      "Iteration 203000, loss=0.07709026336669922\n",
      "Iteration 204000, loss=0.021100923418998718\n",
      "Iteration 205000, loss=0.24655038118362427\n",
      "Iteration 206000, loss=0.004656767938286066\n",
      "Iteration 207000, loss=0.013949098996818066\n",
      "Iteration 208000, loss=0.0\n",
      "Iteration 209000, loss=0.0\n",
      "Iteration 210000, loss=0.010214419104158878\n",
      "Iteration 211000, loss=0.18204854428768158\n",
      "Iteration 212000, loss=0.01883046329021454\n",
      "Iteration 213000, loss=0.9999399185180664\n",
      "Iteration 214000, loss=0.00475451722741127\n",
      "Iteration 215000, loss=1.035971308738226e-11\n",
      "Iteration 216000, loss=0.0\n",
      "Iteration 217000, loss=2.2737367544323206e-13\n",
      "Iteration 218000, loss=0.10862576961517334\n",
      "Iteration 219000, loss=0.0006489910301752388\n",
      "Iteration 220000, loss=0.0009480626904405653\n",
      "Iteration 221000, loss=0.07567255944013596\n",
      "Iteration 222000, loss=0.005074269138276577\n",
      "Iteration 223000, loss=0.004345814231783152\n",
      "Iteration 224000, loss=0.054631344974040985\n",
      "Iteration 225000, loss=0.01617990992963314\n",
      "Iteration 226000, loss=0.0014583816519007087\n",
      "Iteration 227000, loss=0.9999995231628418\n",
      "Iteration 228000, loss=0.0\n",
      "Iteration 229000, loss=0.00534770218655467\n",
      "Iteration 230000, loss=1.0436593811391504e-07\n",
      "Iteration 231000, loss=0.4231722056865692\n",
      "Iteration 232000, loss=0.0\n",
      "Iteration 233000, loss=0.01999419368803501\n",
      "Iteration 234000, loss=0.07988834381103516\n",
      "Iteration 235000, loss=0.0\n",
      "Iteration 236000, loss=0.0\n",
      "Iteration 237000, loss=0.07572958618402481\n",
      "Iteration 238000, loss=0.0012209041742607951\n",
      "Iteration 239000, loss=0.0\n",
      "Iteration 240000, loss=0.0\n",
      "Iteration 241000, loss=0.06590519100427628\n",
      "Iteration 242000, loss=0.5992646217346191\n",
      "Iteration 243000, loss=0.29150643944740295\n",
      "Iteration 244000, loss=0.03764623776078224\n",
      "Iteration 245000, loss=0.06486539542675018\n",
      "Iteration 246000, loss=0.0018302288372069597\n",
      "Iteration 247000, loss=0.07797356694936752\n",
      "Iteration 248000, loss=0.0\n",
      "Iteration 249000, loss=0.00016318853886332363\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# create some input variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)\n",
    "\n",
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "#similarity = merge([target, context], mode='cos', dot_axes=0)\n",
    "#similarity = Dot(axes = 1, normalize = True)([target, context])\n",
    "\n",
    "# now perform the dot product operation to get a similarity measure\n",
    "#dot_product = merge([target, context], mode='dot', dot_axes=1)\n",
    "dot_product = Dot(axes = 1)([target, context])\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "# create the primary training model\n",
    "model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "    \n",
    "    \n",
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "\n",
    "    if cnt % 1000 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "        losses['sutras'].append((cnt,loss))\n",
    "#     if cnt % 8 == 0:\n",
    "#         sim_cb.run_sim()\n",
    "\n",
    "model.save_weights(str('./weights/'+'sutras'+'.h5'))\n",
    "save_obj(losses, 'losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vedas Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5279, 103], [416, 5499], [4353, 2704], [5847, 3730], [5177, 3000], [3551, 487], [3506, 4431], [5177, 4907], [2998, 5284], [2470, 4131]] [0, 0, 0, 0, 1, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "#The size of each vocabulary of each text\n",
    "vocab_size = len(vedas_encoder)\n",
    "\n",
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = skipgrams(vedas_encoded_t, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss=0.24736115336418152\n",
      "Iteration 1000, loss=0.24113285541534424\n",
      "Iteration 2000, loss=0.24639952182769775\n",
      "Iteration 3000, loss=0.2547740936279297\n",
      "Iteration 4000, loss=0.26288464665412903\n",
      "Iteration 5000, loss=0.2423153668642044\n",
      "Iteration 6000, loss=0.2388557493686676\n",
      "Iteration 7000, loss=0.2458762377500534\n",
      "Iteration 8000, loss=0.2385142296552658\n",
      "Iteration 9000, loss=0.2505006790161133\n",
      "Iteration 10000, loss=0.2453857958316803\n",
      "Iteration 11000, loss=0.2425558716058731\n",
      "Iteration 12000, loss=0.25083354115486145\n",
      "Iteration 13000, loss=0.24563181400299072\n",
      "Iteration 14000, loss=0.2528590261936188\n",
      "Iteration 15000, loss=0.2404453158378601\n",
      "Iteration 16000, loss=0.25183531641960144\n",
      "Iteration 17000, loss=0.25450819730758667\n",
      "Iteration 18000, loss=0.2967410981655121\n",
      "Iteration 19000, loss=0.22757062315940857\n",
      "Iteration 20000, loss=0.2662801444530487\n",
      "Iteration 21000, loss=0.23045112192630768\n",
      "Iteration 22000, loss=0.2670726776123047\n",
      "Iteration 23000, loss=0.2222745418548584\n",
      "Iteration 24000, loss=0.27935120463371277\n",
      "Iteration 25000, loss=0.22056269645690918\n",
      "Iteration 26000, loss=0.23376606404781342\n",
      "Iteration 27000, loss=0.23811936378479004\n",
      "Iteration 28000, loss=0.2527710795402527\n",
      "Iteration 29000, loss=0.28323566913604736\n",
      "Iteration 30000, loss=0.25335267186164856\n",
      "Iteration 31000, loss=0.24500083923339844\n",
      "Iteration 32000, loss=0.2603400647640228\n",
      "Iteration 33000, loss=0.2320081889629364\n",
      "Iteration 34000, loss=0.22927144169807434\n",
      "Iteration 35000, loss=0.2658301591873169\n",
      "Iteration 36000, loss=0.23615020513534546\n",
      "Iteration 37000, loss=0.23181810975074768\n",
      "Iteration 38000, loss=0.001806991407647729\n",
      "Iteration 39000, loss=0.23875615000724792\n",
      "Iteration 40000, loss=0.26277557015419006\n",
      "Iteration 41000, loss=0.17532220482826233\n",
      "Iteration 42000, loss=0.2508780360221863\n",
      "Iteration 43000, loss=0.37590089440345764\n",
      "Iteration 44000, loss=0.4176936149597168\n",
      "Iteration 45000, loss=0.25827449560165405\n",
      "Iteration 46000, loss=0.24358654022216797\n",
      "Iteration 47000, loss=0.28526899218559265\n",
      "Iteration 48000, loss=0.002809313125908375\n",
      "Iteration 49000, loss=0.2314806878566742\n",
      "Iteration 50000, loss=0.010370466858148575\n",
      "Iteration 51000, loss=0.2367561310529709\n",
      "Iteration 52000, loss=0.22543565928936005\n",
      "Iteration 53000, loss=1.0\n",
      "Iteration 54000, loss=0.22944539785385132\n",
      "Iteration 55000, loss=0.24399861693382263\n",
      "Iteration 56000, loss=0.22842144966125488\n",
      "Iteration 57000, loss=0.2823619842529297\n",
      "Iteration 58000, loss=0.26161596179008484\n",
      "Iteration 59000, loss=0.21230782568454742\n",
      "Iteration 60000, loss=0.2547789514064789\n",
      "Iteration 61000, loss=0.2921917140483856\n",
      "Iteration 62000, loss=0.21279968321323395\n",
      "Iteration 63000, loss=0.2307061403989792\n",
      "Iteration 64000, loss=0.2530316412448883\n",
      "Iteration 65000, loss=0.2662244141101837\n",
      "Iteration 66000, loss=0.20508477091789246\n",
      "Iteration 67000, loss=1.0\n",
      "Iteration 68000, loss=0.22421908378601074\n",
      "Iteration 69000, loss=0.24189035594463348\n",
      "Iteration 70000, loss=0.2693307399749756\n",
      "Iteration 71000, loss=0.26856061816215515\n",
      "Iteration 72000, loss=0.22131706774234772\n",
      "Iteration 73000, loss=0.2862584590911865\n",
      "Iteration 74000, loss=0.17929445207118988\n",
      "Iteration 75000, loss=0.24193987250328064\n",
      "Iteration 76000, loss=0.272990345954895\n",
      "Iteration 77000, loss=0.25156939029693604\n",
      "Iteration 78000, loss=0.2583739161491394\n",
      "Iteration 79000, loss=0.2531167268753052\n",
      "Iteration 80000, loss=0.20359773933887482\n",
      "Iteration 81000, loss=0.22287024557590485\n",
      "Iteration 82000, loss=0.23840239644050598\n",
      "Iteration 83000, loss=0.32982075214385986\n",
      "Iteration 84000, loss=0.2394210696220398\n",
      "Iteration 85000, loss=0.2746475040912628\n",
      "Iteration 86000, loss=0.23914434015750885\n",
      "Iteration 87000, loss=0.27487221360206604\n",
      "Iteration 88000, loss=0.27471187710762024\n",
      "Iteration 89000, loss=0.00034107224200852215\n",
      "Iteration 90000, loss=0.38861215114593506\n",
      "Iteration 91000, loss=0.22327032685279846\n",
      "Iteration 92000, loss=0.26639026403427124\n",
      "Iteration 93000, loss=0.2144092321395874\n",
      "Iteration 94000, loss=0.22851726412773132\n",
      "Iteration 95000, loss=0.22999528050422668\n",
      "Iteration 96000, loss=0.0008316703024320304\n",
      "Iteration 97000, loss=0.4131588637828827\n",
      "Iteration 98000, loss=0.18168041110038757\n",
      "Iteration 99000, loss=0.23080940544605255\n",
      "Iteration 100000, loss=0.25206178426742554\n",
      "Iteration 101000, loss=0.22092896699905396\n",
      "Iteration 102000, loss=0.22703774273395538\n",
      "Iteration 103000, loss=0.27721622586250305\n",
      "Iteration 104000, loss=0.23698504269123077\n",
      "Iteration 105000, loss=0.09186951071023941\n",
      "Iteration 106000, loss=0.2729674279689789\n",
      "Iteration 107000, loss=0.2109147608280182\n",
      "Iteration 108000, loss=0.36845001578330994\n",
      "Iteration 109000, loss=0.23683680593967438\n",
      "Iteration 110000, loss=0.004237941931933165\n",
      "Iteration 111000, loss=0.13575556874275208\n",
      "Iteration 112000, loss=0.3121618926525116\n",
      "Iteration 113000, loss=0.4487687349319458\n",
      "Iteration 114000, loss=0.279451847076416\n",
      "Iteration 115000, loss=0.2850753366947174\n",
      "Iteration 116000, loss=0.248897522687912\n",
      "Iteration 117000, loss=0.08827246725559235\n",
      "Iteration 118000, loss=0.19926483929157257\n",
      "Iteration 119000, loss=0.27683693170547485\n",
      "Iteration 120000, loss=0.4442500174045563\n",
      "Iteration 121000, loss=0.2071167677640915\n",
      "Iteration 122000, loss=0.294452965259552\n",
      "Iteration 123000, loss=0.2728986144065857\n",
      "Iteration 124000, loss=0.20418721437454224\n",
      "Iteration 125000, loss=0.26718759536743164\n",
      "Iteration 126000, loss=0.11929295212030411\n",
      "Iteration 127000, loss=0.1711375117301941\n",
      "Iteration 128000, loss=0.24060191214084625\n",
      "Iteration 129000, loss=0.23165689408779144\n",
      "Iteration 130000, loss=0.20641997456550598\n",
      "Iteration 131000, loss=0.14994384348392487\n",
      "Iteration 132000, loss=0.2777470350265503\n",
      "Iteration 133000, loss=0.34690287709236145\n",
      "Iteration 134000, loss=0.2090134173631668\n",
      "Iteration 135000, loss=0.21222145855426788\n",
      "Iteration 136000, loss=0.21224896609783173\n",
      "Iteration 137000, loss=0.30422571301460266\n",
      "Iteration 138000, loss=0.26550784707069397\n",
      "Iteration 139000, loss=0.19127815961837769\n",
      "Iteration 140000, loss=0.2160515934228897\n",
      "Iteration 141000, loss=0.2017529010772705\n",
      "Iteration 142000, loss=0.3220435380935669\n",
      "Iteration 143000, loss=0.29423215985298157\n",
      "Iteration 144000, loss=0.2867545485496521\n",
      "Iteration 145000, loss=0.21554028987884521\n",
      "Iteration 146000, loss=0.30036407709121704\n",
      "Iteration 147000, loss=0.21364492177963257\n",
      "Iteration 148000, loss=0.2396842986345291\n",
      "Iteration 149000, loss=0.0006808885373175144\n",
      "Iteration 150000, loss=0.22160831093788147\n",
      "Iteration 151000, loss=0.22217679023742676\n",
      "Iteration 152000, loss=0.2131805568933487\n",
      "Iteration 153000, loss=0.2953701615333557\n",
      "Iteration 154000, loss=0.1776116043329239\n",
      "Iteration 155000, loss=0.6967933177947998\n",
      "Iteration 156000, loss=0.0006745359278284013\n",
      "Iteration 157000, loss=0.19873164594173431\n",
      "Iteration 158000, loss=0.1949472874403\n",
      "Iteration 159000, loss=0.3145427107810974\n",
      "Iteration 160000, loss=0.25786784291267395\n",
      "Iteration 161000, loss=0.2096894085407257\n",
      "Iteration 162000, loss=0.30972060561180115\n",
      "Iteration 163000, loss=0.3107244372367859\n",
      "Iteration 164000, loss=0.20280230045318604\n",
      "Iteration 165000, loss=0.294836163520813\n",
      "Iteration 166000, loss=0.19182088971138\n",
      "Iteration 167000, loss=0.28085851669311523\n",
      "Iteration 168000, loss=0.30271995067596436\n",
      "Iteration 169000, loss=0.20775361359119415\n",
      "Iteration 170000, loss=0.2977795898914337\n",
      "Iteration 171000, loss=0.014558305032551289\n",
      "Iteration 172000, loss=0.36674052476882935\n",
      "Iteration 173000, loss=0.20642200112342834\n",
      "Iteration 174000, loss=0.00014300808834377676\n",
      "Iteration 175000, loss=0.21026542782783508\n",
      "Iteration 176000, loss=0.22571825981140137\n",
      "Iteration 177000, loss=0.3172484338283539\n",
      "Iteration 178000, loss=0.13236019015312195\n",
      "Iteration 179000, loss=0.21862725913524628\n",
      "Iteration 180000, loss=0.4252612590789795\n",
      "Iteration 181000, loss=0.21656078100204468\n",
      "Iteration 182000, loss=0.22365374863147736\n",
      "Iteration 183000, loss=0.25937458872795105\n",
      "Iteration 184000, loss=0.18540915846824646\n",
      "Iteration 185000, loss=0.20855994522571564\n",
      "Iteration 186000, loss=0.22838416695594788\n",
      "Iteration 187000, loss=0.20293410122394562\n",
      "Iteration 188000, loss=0.3146146535873413\n",
      "Iteration 189000, loss=0.26233577728271484\n",
      "Iteration 190000, loss=0.29273635149002075\n",
      "Iteration 191000, loss=0.296581894159317\n",
      "Iteration 192000, loss=0.2995847761631012\n",
      "Iteration 193000, loss=0.29131004214286804\n",
      "Iteration 194000, loss=1.0\n",
      "Iteration 195000, loss=0.11272469162940979\n",
      "Iteration 196000, loss=0.004286903887987137\n",
      "Iteration 197000, loss=0.47285303473472595\n",
      "Iteration 198000, loss=0.09090589731931686\n",
      "Iteration 199000, loss=0.21822160482406616\n",
      "Iteration 200000, loss=0.3202449381351471\n",
      "Iteration 201000, loss=0.21954655647277832\n",
      "Iteration 202000, loss=0.13576360046863556\n",
      "Iteration 203000, loss=0.2854481637477875\n",
      "Iteration 204000, loss=0.29764261841773987\n",
      "Iteration 205000, loss=0.2891651391983032\n",
      "Iteration 206000, loss=0.25404563546180725\n",
      "Iteration 207000, loss=0.01189674623310566\n",
      "Iteration 208000, loss=0.36741626262664795\n",
      "Iteration 209000, loss=0.3852654695510864\n",
      "Iteration 210000, loss=0.0012825733283534646\n",
      "Iteration 211000, loss=0.0018109765369445086\n",
      "Iteration 212000, loss=0.21935038268566132\n",
      "Iteration 213000, loss=0.2963404357433319\n",
      "Iteration 214000, loss=0.6459922790527344\n",
      "Iteration 215000, loss=0.28868794441223145\n",
      "Iteration 216000, loss=0.006386052817106247\n",
      "Iteration 217000, loss=0.21221885085105896\n",
      "Iteration 218000, loss=0.2883974313735962\n",
      "Iteration 219000, loss=0.2960782051086426\n",
      "Iteration 220000, loss=0.3458397686481476\n",
      "Iteration 221000, loss=0.07673316448926926\n",
      "Iteration 222000, loss=0.3040672838687897\n",
      "Iteration 223000, loss=0.18387974798679352\n",
      "Iteration 224000, loss=0.20383325219154358\n",
      "Iteration 225000, loss=0.20984689891338348\n",
      "Iteration 226000, loss=0.20491251349449158\n",
      "Iteration 227000, loss=0.00011888452718267217\n",
      "Iteration 228000, loss=0.18828743696212769\n",
      "Iteration 229000, loss=0.19389693439006805\n",
      "Iteration 230000, loss=0.2907979190349579\n",
      "Iteration 231000, loss=0.22258874773979187\n",
      "Iteration 232000, loss=0.2966841459274292\n",
      "Iteration 233000, loss=0.1302884966135025\n",
      "Iteration 234000, loss=0.28762516379356384\n",
      "Iteration 235000, loss=0.2320990115404129\n",
      "Iteration 236000, loss=0.2051461786031723\n",
      "Iteration 237000, loss=0.18152658641338348\n",
      "Iteration 238000, loss=0.20045410096645355\n",
      "Iteration 239000, loss=0.20162704586982727\n",
      "Iteration 240000, loss=0.0001633698120713234\n",
      "Iteration 241000, loss=0.26839396357536316\n",
      "Iteration 242000, loss=0.1868046373128891\n",
      "Iteration 243000, loss=0.3422592878341675\n",
      "Iteration 244000, loss=0.23281230032444\n",
      "Iteration 245000, loss=0.2846580743789673\n",
      "Iteration 246000, loss=0.2020072191953659\n",
      "Iteration 247000, loss=0.3047211468219757\n",
      "Iteration 248000, loss=0.20275777578353882\n",
      "Iteration 249000, loss=0.18994778394699097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create some input variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)\n",
    "\n",
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "#similarity = merge([target, context], mode='cos', dot_axes=0)\n",
    "#similarity = Dot(axes = 1, normalize = True)([target, context])\n",
    "\n",
    "# now perform the dot product operation to get a similarity measure\n",
    "#dot_product = merge([target, context], mode='dot', dot_axes=1)\n",
    "dot_product = Dot(axes = 1)([target, context])\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "# create the primary training model\n",
    "model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "    \n",
    "    \n",
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "\n",
    "    if cnt % 1000 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "        losses['vedas'].append((cnt,loss))\n",
    "#     if cnt % 8 == 0:\n",
    "#         sim_cb.run_sim()\n",
    "\n",
    "model.save_weights(str('./weights/'+'vedas'+'.h5'))\n",
    "save_obj(losses, 'losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bible Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6177, 1642], [6217, 4286], [6177, 4703], [3723, 2977], [1778, 2918], [4776, 385], [6276, 6607], [4703, 5845], [3628, 6757], [4567, 6181]] [0, 1, 1, 0, 1, 0, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "#The size of each vocabulary of each text\n",
    "vocab_size = len(bible_encoder)\n",
    "\n",
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = skipgrams(bible_encoded_t, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss=0.2497856169939041\n",
      "Iteration 1000, loss=0.24721525609493256\n",
      "Iteration 2000, loss=0.23643669486045837\n",
      "Iteration 3000, loss=0.25573065876960754\n",
      "Iteration 4000, loss=0.2564864754676819\n",
      "Iteration 5000, loss=0.26956576108932495\n",
      "Iteration 6000, loss=0.2736494541168213\n",
      "Iteration 7000, loss=0.21736405789852142\n",
      "Iteration 8000, loss=0.29998645186424255\n",
      "Iteration 9000, loss=0.23716172575950623\n",
      "Iteration 10000, loss=0.1738279014825821\n",
      "Iteration 11000, loss=0.3043056130409241\n",
      "Iteration 12000, loss=0.07531508058309555\n",
      "Iteration 13000, loss=0.41526609659194946\n",
      "Iteration 14000, loss=0.11638853698968887\n",
      "Iteration 15000, loss=0.26964467763900757\n",
      "Iteration 16000, loss=0.11312323808670044\n",
      "Iteration 17000, loss=0.131582111120224\n",
      "Iteration 18000, loss=0.0\n",
      "Iteration 19000, loss=0.008011546917259693\n",
      "Iteration 20000, loss=0.11139557510614395\n",
      "Iteration 21000, loss=0.12406782805919647\n",
      "Iteration 22000, loss=0.014561692252755165\n",
      "Iteration 23000, loss=0.0\n",
      "Iteration 24000, loss=0.1121792271733284\n",
      "Iteration 25000, loss=0.0566897951066494\n",
      "Iteration 26000, loss=0.5973389148712158\n",
      "Iteration 27000, loss=0.0811782032251358\n",
      "Iteration 28000, loss=0.0511210598051548\n",
      "Iteration 29000, loss=0.0\n",
      "Iteration 30000, loss=0.0\n",
      "Iteration 31000, loss=1.2789769243681803e-13\n",
      "Iteration 32000, loss=0.039574481546878815\n",
      "Iteration 33000, loss=0.08378918468952179\n",
      "Iteration 34000, loss=0.06367310881614685\n",
      "Iteration 35000, loss=0.028760353103280067\n",
      "Iteration 36000, loss=0.0\n",
      "Iteration 37000, loss=0.578173816204071\n",
      "Iteration 38000, loss=0.0\n",
      "Iteration 39000, loss=0.07137592881917953\n",
      "Iteration 40000, loss=0.003909730352461338\n",
      "Iteration 41000, loss=0.05617513880133629\n",
      "Iteration 42000, loss=0.0\n",
      "Iteration 43000, loss=0.12408220767974854\n",
      "Iteration 44000, loss=0.028750235214829445\n",
      "Iteration 45000, loss=0.0649326965212822\n",
      "Iteration 46000, loss=0.19692400097846985\n",
      "Iteration 47000, loss=0.0012742821127176285\n",
      "Iteration 48000, loss=0.06812132894992828\n",
      "Iteration 49000, loss=0.10374891757965088\n",
      "Iteration 50000, loss=0.03318728879094124\n",
      "Iteration 51000, loss=0.9997823238372803\n",
      "Iteration 52000, loss=0.06452745199203491\n",
      "Iteration 53000, loss=0.6535958051681519\n",
      "Iteration 54000, loss=0.031592585146427155\n",
      "Iteration 55000, loss=0.0\n",
      "Iteration 56000, loss=0.0011946620652452111\n",
      "Iteration 57000, loss=0.01250440627336502\n",
      "Iteration 58000, loss=3.946766469198337e-09\n",
      "Iteration 59000, loss=0.0\n",
      "Iteration 60000, loss=0.013524497859179974\n",
      "Iteration 61000, loss=0.0040199169889092445\n",
      "Iteration 62000, loss=0.029527541249990463\n",
      "Iteration 63000, loss=0.9999990463256836\n",
      "Iteration 64000, loss=0.08605370670557022\n",
      "Iteration 65000, loss=0.9732733368873596\n",
      "Iteration 66000, loss=0.03539837896823883\n",
      "Iteration 67000, loss=0.000808042474091053\n",
      "Iteration 68000, loss=0.001827298547141254\n",
      "Iteration 69000, loss=0.057554323226213455\n",
      "Iteration 70000, loss=0.0350503996014595\n",
      "Iteration 71000, loss=0.032210823148489\n",
      "Iteration 72000, loss=0.020833216607570648\n",
      "Iteration 73000, loss=0.0\n",
      "Iteration 74000, loss=1.2789769243681803e-13\n",
      "Iteration 75000, loss=0.0\n",
      "Iteration 76000, loss=0.0\n",
      "Iteration 77000, loss=0.17905758321285248\n",
      "Iteration 78000, loss=0.034660983830690384\n",
      "Iteration 79000, loss=0.49989452958106995\n",
      "Iteration 80000, loss=0.0\n",
      "Iteration 81000, loss=0.06641466170549393\n",
      "Iteration 82000, loss=0.0017184256576001644\n",
      "Iteration 83000, loss=3.5254040540166898e-06\n",
      "Iteration 84000, loss=0.025302132591605186\n",
      "Iteration 85000, loss=1.0903846714427345e-09\n",
      "Iteration 86000, loss=0.06775254011154175\n",
      "Iteration 87000, loss=0.04405128210783005\n",
      "Iteration 88000, loss=0.0002968227199744433\n",
      "Iteration 89000, loss=0.007345415651798248\n",
      "Iteration 90000, loss=1.1201420420547947e-05\n",
      "Iteration 91000, loss=0.00047083376557566226\n",
      "Iteration 92000, loss=0.03754507005214691\n",
      "Iteration 93000, loss=0.0\n",
      "Iteration 94000, loss=0.17998258769512177\n",
      "Iteration 95000, loss=0.06684025377035141\n",
      "Iteration 96000, loss=3.1391778065881226e-11\n",
      "Iteration 97000, loss=0.007545007858425379\n",
      "Iteration 98000, loss=0.13604147732257843\n",
      "Iteration 99000, loss=0.9910574555397034\n",
      "Iteration 100000, loss=0.9999954700469971\n",
      "Iteration 101000, loss=0.00010878479952225462\n",
      "Iteration 102000, loss=0.019020330160856247\n",
      "Iteration 103000, loss=2.0978419001949078e-08\n",
      "Iteration 104000, loss=0.0\n",
      "Iteration 105000, loss=0.005916341673582792\n",
      "Iteration 106000, loss=0.6831101179122925\n",
      "Iteration 107000, loss=0.0014083617134019732\n",
      "Iteration 108000, loss=0.0\n",
      "Iteration 109000, loss=0.05833900719881058\n",
      "Iteration 110000, loss=0.6388581395149231\n",
      "Iteration 111000, loss=0.9942374229431152\n",
      "Iteration 112000, loss=0.5946555137634277\n",
      "Iteration 113000, loss=5.684341886080802e-14\n",
      "Iteration 114000, loss=2.1614710021822248e-11\n",
      "Iteration 115000, loss=0.6505829691886902\n",
      "Iteration 116000, loss=0.7310965657234192\n",
      "Iteration 117000, loss=0.10664703696966171\n",
      "Iteration 118000, loss=0.5404244661331177\n",
      "Iteration 119000, loss=0.3316621482372284\n",
      "Iteration 120000, loss=5.581920959230047e-06\n",
      "Iteration 121000, loss=0.018895655870437622\n",
      "Iteration 122000, loss=0.8141278624534607\n",
      "Iteration 123000, loss=0.00020353157015051693\n",
      "Iteration 124000, loss=0.025994006544351578\n",
      "Iteration 125000, loss=0.0\n",
      "Iteration 126000, loss=0.0\n",
      "Iteration 127000, loss=0.00880223698914051\n",
      "Iteration 128000, loss=0.08222051709890366\n",
      "Iteration 129000, loss=0.0\n",
      "Iteration 130000, loss=0.5315848588943481\n",
      "Iteration 131000, loss=0.02067815512418747\n",
      "Iteration 132000, loss=0.06274285912513733\n",
      "Iteration 133000, loss=0.0\n",
      "Iteration 134000, loss=0.5668225288391113\n",
      "Iteration 135000, loss=0.0221774410456419\n",
      "Iteration 136000, loss=0.23981349170207977\n",
      "Iteration 137000, loss=0.036117713898420334\n",
      "Iteration 138000, loss=0.0\n",
      "Iteration 139000, loss=0.023871196433901787\n",
      "Iteration 140000, loss=0.0\n",
      "Iteration 141000, loss=0.4793160557746887\n",
      "Iteration 142000, loss=0.0\n",
      "Iteration 143000, loss=0.04528152197599411\n",
      "Iteration 144000, loss=0.0031953989528119564\n",
      "Iteration 145000, loss=0.0667807087302208\n",
      "Iteration 146000, loss=0.05915386602282524\n",
      "Iteration 147000, loss=0.35575392842292786\n",
      "Iteration 148000, loss=0.007835629396140575\n",
      "Iteration 149000, loss=9.606537787476555e-12\n",
      "Iteration 150000, loss=0.4567880928516388\n",
      "Iteration 151000, loss=0.07316019386053085\n",
      "Iteration 152000, loss=0.02630607597529888\n",
      "Iteration 153000, loss=3.6817624504692503e-09\n",
      "Iteration 154000, loss=0.023492878302931786\n",
      "Iteration 155000, loss=0.3875305950641632\n",
      "Iteration 156000, loss=0.0047097960487008095\n",
      "Iteration 157000, loss=0.556465744972229\n",
      "Iteration 158000, loss=0.0\n",
      "Iteration 159000, loss=0.0\n",
      "Iteration 160000, loss=0.5660250186920166\n",
      "Iteration 161000, loss=0.058102287352085114\n",
      "Iteration 162000, loss=0.03149138391017914\n",
      "Iteration 163000, loss=0.5231440663337708\n",
      "Iteration 164000, loss=1.4410503013095877e-08\n",
      "Iteration 165000, loss=0.07425519824028015\n",
      "Iteration 166000, loss=0.08921533823013306\n",
      "Iteration 167000, loss=0.03307546675205231\n",
      "Iteration 168000, loss=0.0\n",
      "Iteration 169000, loss=0.0006369804032146931\n",
      "Iteration 170000, loss=0.05972515791654587\n",
      "Iteration 171000, loss=0.04485582932829857\n",
      "Iteration 172000, loss=0.9778214693069458\n",
      "Iteration 173000, loss=0.02603187970817089\n",
      "Iteration 174000, loss=0.07239095121622086\n",
      "Iteration 175000, loss=0.0\n",
      "Iteration 176000, loss=0.0\n",
      "Iteration 177000, loss=0.0\n",
      "Iteration 178000, loss=0.9214009642601013\n",
      "Iteration 179000, loss=0.3630819022655487\n",
      "Iteration 180000, loss=0.0\n",
      "Iteration 181000, loss=3.573240519472165e-06\n",
      "Iteration 182000, loss=0.0\n",
      "Iteration 183000, loss=0.006551690399646759\n",
      "Iteration 184000, loss=0.008465860038995743\n",
      "Iteration 185000, loss=0.027820099145174026\n",
      "Iteration 186000, loss=0.660249650478363\n",
      "Iteration 187000, loss=1.5475620784854982e-11\n",
      "Iteration 188000, loss=0.016302019357681274\n",
      "Iteration 189000, loss=0.6131075620651245\n",
      "Iteration 190000, loss=0.02931174822151661\n",
      "Iteration 191000, loss=2.8860173983957793e-07\n",
      "Iteration 192000, loss=0.0\n",
      "Iteration 193000, loss=0.0\n",
      "Iteration 194000, loss=0.0\n",
      "Iteration 195000, loss=0.7193054556846619\n",
      "Iteration 196000, loss=0.004185936879366636\n",
      "Iteration 197000, loss=0.03863450139760971\n",
      "Iteration 198000, loss=1.134918079515046e-07\n",
      "Iteration 199000, loss=0.0\n",
      "Iteration 200000, loss=0.04107983037829399\n",
      "Iteration 201000, loss=0.00028394756373018026\n",
      "Iteration 202000, loss=0.7871902585029602\n",
      "Iteration 203000, loss=0.0\n",
      "Iteration 204000, loss=0.0\n",
      "Iteration 205000, loss=0.01769675873219967\n",
      "Iteration 206000, loss=0.9647392630577087\n",
      "Iteration 207000, loss=0.3406188488006592\n",
      "Iteration 208000, loss=6.37925268165418e-11\n",
      "Iteration 209000, loss=0.9506020545959473\n",
      "Iteration 210000, loss=8.881784197001252e-12\n",
      "Iteration 211000, loss=0.0064812153577804565\n",
      "Iteration 212000, loss=0.05279742553830147\n",
      "Iteration 213000, loss=0.08194117248058319\n",
      "Iteration 214000, loss=9.094947017729282e-13\n",
      "Iteration 215000, loss=0.04469418153166771\n",
      "Iteration 216000, loss=0.1667904555797577\n",
      "Iteration 217000, loss=0.028587881475687027\n",
      "Iteration 218000, loss=0.08621065318584442\n",
      "Iteration 219000, loss=0.10477863252162933\n",
      "Iteration 220000, loss=0.015301959589123726\n",
      "Iteration 221000, loss=0.028472375124692917\n",
      "Iteration 222000, loss=0.7810028195381165\n",
      "Iteration 223000, loss=1.68839164871315e-10\n",
      "Iteration 224000, loss=0.0198502354323864\n",
      "Iteration 225000, loss=0.022747796028852463\n",
      "Iteration 226000, loss=0.022837018594145775\n",
      "Iteration 227000, loss=0.005448041949421167\n",
      "Iteration 228000, loss=0.022568346932530403\n",
      "Iteration 229000, loss=1.176800878965878e-10\n",
      "Iteration 230000, loss=0.0017836415208876133\n",
      "Iteration 231000, loss=0.0\n",
      "Iteration 232000, loss=0.019982758909463882\n",
      "Iteration 233000, loss=4.7204703150782734e-05\n",
      "Iteration 234000, loss=0.3625143766403198\n",
      "Iteration 235000, loss=0.03307630121707916\n",
      "Iteration 236000, loss=0.004546904005110264\n",
      "Iteration 237000, loss=0.003140747547149658\n",
      "Iteration 238000, loss=0.04278968274593353\n",
      "Iteration 239000, loss=0.0134148970246315\n",
      "Iteration 240000, loss=2.4016344468691386e-12\n",
      "Iteration 241000, loss=0.0022146101109683514\n",
      "Iteration 242000, loss=0.009837165474891663\n",
      "Iteration 243000, loss=2.7853275241795927e-12\n",
      "Iteration 244000, loss=0.08234433084726334\n",
      "Iteration 245000, loss=8.185452315956354e-12\n",
      "Iteration 246000, loss=0.07392340153455734\n",
      "Iteration 247000, loss=0.5091596841812134\n",
      "Iteration 248000, loss=0.0\n",
      "Iteration 249000, loss=0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create some input variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)\n",
    "\n",
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "#similarity = merge([target, context], mode='cos', dot_axes=0)\n",
    "#similarity = Dot(axes = 1, normalize = True)([target, context])\n",
    "\n",
    "# now perform the dot product operation to get a similarity measure\n",
    "#dot_product = merge([target, context], mode='dot', dot_axes=1)\n",
    "dot_product = Dot(axes = 1)([target, context])\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "# create the primary training model\n",
    "model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "    \n",
    "    \n",
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "\n",
    "    if cnt % 1000 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "        losses['bible'].append((cnt,loss))\n",
    "#     if cnt % 8 == 0:\n",
    "#         sim_cb.run_sim()\n",
    "\n",
    "model.save_weights(str('./weights/'+'bible'+'.h5'))\n",
    "save_obj(losses, 'losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanakh Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[182, 585], [610, 979], [1026, 2149], [1672, 1155], [2054, 1415], [2234, 1180], [646, 1280], [1805, 538], [963, 1026], [843, 1672]] [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#The size of each vocabulary of each text\n",
    "vocab_size = len(tanakh_encoder)\n",
    "\n",
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = skipgrams(tanakh_encoded_t, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss=0.24872258305549622\n",
      "Iteration 1000, loss=0.23844502866268158\n",
      "Iteration 2000, loss=0.2441060096025467\n",
      "Iteration 3000, loss=0.24100995063781738\n",
      "Iteration 4000, loss=0.24062496423721313\n",
      "Iteration 5000, loss=0.24936150014400482\n",
      "Iteration 6000, loss=0.2466319501399994\n",
      "Iteration 7000, loss=0.25130078196525574\n",
      "Iteration 8000, loss=0.23519419133663177\n",
      "Iteration 9000, loss=0.24238820374011993\n",
      "Iteration 10000, loss=0.2606649696826935\n",
      "Iteration 11000, loss=0.24435916543006897\n",
      "Iteration 12000, loss=0.24872031807899475\n",
      "Iteration 13000, loss=0.24735382199287415\n",
      "Iteration 14000, loss=0.24655583500862122\n",
      "Iteration 15000, loss=0.21827882528305054\n",
      "Iteration 16000, loss=0.26841893792152405\n",
      "Iteration 17000, loss=0.26090988516807556\n",
      "Iteration 18000, loss=0.23142556846141815\n",
      "Iteration 19000, loss=0.24977387487888336\n",
      "Iteration 20000, loss=0.2601797580718994\n",
      "Iteration 21000, loss=0.23067401349544525\n",
      "Iteration 22000, loss=0.21722754836082458\n",
      "Iteration 23000, loss=0.21048080921173096\n",
      "Iteration 24000, loss=0.009529619477689266\n",
      "Iteration 25000, loss=0.4210582673549652\n",
      "Iteration 26000, loss=0.2930457592010498\n",
      "Iteration 27000, loss=0.23995545506477356\n",
      "Iteration 28000, loss=0.2591921091079712\n",
      "Iteration 29000, loss=0.2546694576740265\n",
      "Iteration 30000, loss=0.23428182303905487\n",
      "Iteration 31000, loss=0.2753921449184418\n",
      "Iteration 32000, loss=0.2335166484117508\n",
      "Iteration 33000, loss=0.2273130565881729\n",
      "Iteration 34000, loss=0.31671398878097534\n",
      "Iteration 35000, loss=0.22163407504558563\n",
      "Iteration 36000, loss=0.004413226153701544\n",
      "Iteration 37000, loss=0.20670254528522491\n",
      "Iteration 38000, loss=0.15372149646282196\n",
      "Iteration 39000, loss=0.20670194923877716\n",
      "Iteration 40000, loss=0.28669607639312744\n",
      "Iteration 41000, loss=0.005061381962150335\n",
      "Iteration 42000, loss=0.22179104387760162\n",
      "Iteration 43000, loss=0.29597073793411255\n",
      "Iteration 44000, loss=0.21935665607452393\n",
      "Iteration 45000, loss=0.3269052803516388\n",
      "Iteration 46000, loss=0.20429839193820953\n",
      "Iteration 47000, loss=0.289873331785202\n",
      "Iteration 48000, loss=0.0016078362241387367\n",
      "Iteration 49000, loss=0.2197781652212143\n",
      "Iteration 50000, loss=0.09095866233110428\n",
      "Iteration 51000, loss=0.24071794748306274\n",
      "Iteration 52000, loss=0.06935504823923111\n",
      "Iteration 53000, loss=0.27425870299339294\n",
      "Iteration 54000, loss=0.5680095553398132\n",
      "Iteration 55000, loss=0.18703080713748932\n",
      "Iteration 56000, loss=0.0017076351214200258\n",
      "Iteration 57000, loss=0.0002520623675081879\n",
      "Iteration 58000, loss=0.0001855503796832636\n",
      "Iteration 59000, loss=0.20561276376247406\n",
      "Iteration 60000, loss=0.2086208164691925\n",
      "Iteration 61000, loss=0.0013700500130653381\n",
      "Iteration 62000, loss=0.20815379917621613\n",
      "Iteration 63000, loss=0.4890022277832031\n",
      "Iteration 64000, loss=0.21145915985107422\n",
      "Iteration 65000, loss=0.2907066345214844\n",
      "Iteration 66000, loss=0.2108003944158554\n",
      "Iteration 67000, loss=0.19134289026260376\n",
      "Iteration 68000, loss=0.2958211898803711\n",
      "Iteration 69000, loss=0.2053908258676529\n",
      "Iteration 70000, loss=0.1871500462293625\n",
      "Iteration 71000, loss=0.30845752358436584\n",
      "Iteration 72000, loss=0.1626417487859726\n",
      "Iteration 73000, loss=0.20396623015403748\n",
      "Iteration 74000, loss=0.18451914191246033\n",
      "Iteration 75000, loss=0.3441367745399475\n",
      "Iteration 76000, loss=0.40402352809906006\n",
      "Iteration 77000, loss=0.19456778466701508\n",
      "Iteration 78000, loss=0.20059195160865784\n",
      "Iteration 79000, loss=0.14736053347587585\n",
      "Iteration 80000, loss=0.08321158587932587\n",
      "Iteration 81000, loss=0.0011436777422204614\n",
      "Iteration 82000, loss=0.32717475295066833\n",
      "Iteration 83000, loss=0.19742533564567566\n",
      "Iteration 84000, loss=0.2597251236438751\n",
      "Iteration 85000, loss=0.21598435938358307\n",
      "Iteration 86000, loss=0.3088187277317047\n",
      "Iteration 87000, loss=0.21553935110569\n",
      "Iteration 88000, loss=0.20348267257213593\n",
      "Iteration 89000, loss=0.29523465037345886\n",
      "Iteration 90000, loss=0.1982620507478714\n",
      "Iteration 91000, loss=0.38789528608322144\n",
      "Iteration 92000, loss=0.006965278647840023\n",
      "Iteration 93000, loss=0.3275463581085205\n",
      "Iteration 94000, loss=0.34267011284828186\n",
      "Iteration 95000, loss=0.2163422554731369\n",
      "Iteration 96000, loss=0.2936214208602905\n",
      "Iteration 97000, loss=0.20034587383270264\n",
      "Iteration 98000, loss=0.77180016040802\n",
      "Iteration 99000, loss=0.19817809760570526\n",
      "Iteration 100000, loss=0.0857880488038063\n",
      "Iteration 101000, loss=0.2129957526922226\n",
      "Iteration 102000, loss=0.31877827644348145\n",
      "Iteration 103000, loss=0.20998980104923248\n",
      "Iteration 104000, loss=0.2927480936050415\n",
      "Iteration 105000, loss=0.2932382822036743\n",
      "Iteration 106000, loss=0.19383610785007477\n",
      "Iteration 107000, loss=0.301794171333313\n",
      "Iteration 108000, loss=0.2777964174747467\n",
      "Iteration 109000, loss=0.17331568896770477\n",
      "Iteration 110000, loss=0.16246193647384644\n",
      "Iteration 111000, loss=0.21096399426460266\n",
      "Iteration 112000, loss=0.3270508646965027\n",
      "Iteration 113000, loss=0.23049907386302948\n",
      "Iteration 114000, loss=0.17942672967910767\n",
      "Iteration 115000, loss=0.20313289761543274\n",
      "Iteration 116000, loss=0.3049037456512451\n",
      "Iteration 117000, loss=0.19930732250213623\n",
      "Iteration 118000, loss=0.19176293909549713\n",
      "Iteration 119000, loss=0.18825888633728027\n",
      "Iteration 120000, loss=0.1868213266134262\n",
      "Iteration 121000, loss=0.3743876516819\n",
      "Iteration 122000, loss=0.42194071412086487\n",
      "Iteration 123000, loss=0.19516843557357788\n",
      "Iteration 124000, loss=0.1944984346628189\n",
      "Iteration 125000, loss=0.1955917924642563\n",
      "Iteration 126000, loss=0.1920657604932785\n",
      "Iteration 127000, loss=0.13451054692268372\n",
      "Iteration 128000, loss=0.19770723581314087\n",
      "Iteration 129000, loss=0.932990312576294\n",
      "Iteration 130000, loss=0.30439668893814087\n",
      "Iteration 131000, loss=0.34430280327796936\n",
      "Iteration 132000, loss=0.17161260545253754\n",
      "Iteration 133000, loss=0.32542023062705994\n",
      "Iteration 134000, loss=0.37671494483947754\n",
      "Iteration 135000, loss=0.0014245094498619437\n",
      "Iteration 136000, loss=0.2656266391277313\n",
      "Iteration 137000, loss=0.4890855848789215\n",
      "Iteration 138000, loss=0.18713214993476868\n",
      "Iteration 139000, loss=0.43215519189834595\n",
      "Iteration 140000, loss=0.18754364550113678\n",
      "Iteration 141000, loss=0.1879781037569046\n",
      "Iteration 142000, loss=0.2901569604873657\n",
      "Iteration 143000, loss=0.07924462109804153\n",
      "Iteration 144000, loss=0.2414710372686386\n",
      "Iteration 145000, loss=0.08991428464651108\n",
      "Iteration 146000, loss=0.21008893847465515\n",
      "Iteration 147000, loss=0.0003448960487730801\n",
      "Iteration 148000, loss=0.17486238479614258\n",
      "Iteration 149000, loss=0.7268361449241638\n",
      "Iteration 150000, loss=0.012133858166635036\n",
      "Iteration 151000, loss=0.3485441207885742\n",
      "Iteration 152000, loss=0.005162105895578861\n",
      "Iteration 153000, loss=0.16006796061992645\n",
      "Iteration 154000, loss=0.18723726272583008\n",
      "Iteration 155000, loss=0.2938670516014099\n",
      "Iteration 156000, loss=0.3328116834163666\n",
      "Iteration 157000, loss=0.17934730648994446\n",
      "Iteration 158000, loss=0.3709670603275299\n",
      "Iteration 159000, loss=0.3158278167247772\n",
      "Iteration 160000, loss=0.18378907442092896\n",
      "Iteration 161000, loss=0.31068459153175354\n",
      "Iteration 162000, loss=0.18287359178066254\n",
      "Iteration 163000, loss=0.0029996149241924286\n",
      "Iteration 164000, loss=0.2768687605857849\n",
      "Iteration 165000, loss=0.17805078625679016\n",
      "Iteration 166000, loss=0.12692934274673462\n",
      "Iteration 167000, loss=0.20568028092384338\n",
      "Iteration 168000, loss=0.18764519691467285\n",
      "Iteration 169000, loss=0.1863822191953659\n",
      "Iteration 170000, loss=0.4398556053638458\n",
      "Iteration 171000, loss=0.024240994825959206\n",
      "Iteration 172000, loss=0.17238183319568634\n",
      "Iteration 173000, loss=0.17315968871116638\n",
      "Iteration 174000, loss=0.3325119912624359\n",
      "Iteration 175000, loss=0.7382962107658386\n",
      "Iteration 176000, loss=0.37351325154304504\n",
      "Iteration 177000, loss=0.6250844597816467\n",
      "Iteration 178000, loss=0.17839936912059784\n",
      "Iteration 179000, loss=0.3618015646934509\n",
      "Iteration 180000, loss=0.3489223122596741\n",
      "Iteration 181000, loss=0.17062832415103912\n",
      "Iteration 182000, loss=0.17149047553539276\n",
      "Iteration 183000, loss=0.3593437969684601\n",
      "Iteration 184000, loss=0.18694821000099182\n",
      "Iteration 185000, loss=0.15190014243125916\n",
      "Iteration 186000, loss=0.3355834186077118\n",
      "Iteration 187000, loss=0.20204691588878632\n",
      "Iteration 188000, loss=0.27208012342453003\n",
      "Iteration 189000, loss=0.2277446985244751\n",
      "Iteration 190000, loss=0.1738990694284439\n",
      "Iteration 191000, loss=0.09052173793315887\n",
      "Iteration 192000, loss=0.003389974357560277\n",
      "Iteration 193000, loss=0.3908768594264984\n",
      "Iteration 194000, loss=0.378824383020401\n",
      "Iteration 195000, loss=0.25872331857681274\n",
      "Iteration 196000, loss=0.1715720146894455\n",
      "Iteration 197000, loss=0.1947687864303589\n",
      "Iteration 198000, loss=1.0\n",
      "Iteration 199000, loss=0.3949521481990814\n",
      "Iteration 200000, loss=0.18008412420749664\n",
      "Iteration 201000, loss=0.1714266538619995\n",
      "Iteration 202000, loss=0.17962141335010529\n",
      "Iteration 203000, loss=0.22572794556617737\n",
      "Iteration 204000, loss=0.17730334401130676\n",
      "Iteration 205000, loss=0.3424229919910431\n",
      "Iteration 206000, loss=0.2245875597000122\n",
      "Iteration 207000, loss=0.17282114923000336\n",
      "Iteration 208000, loss=0.3991954028606415\n",
      "Iteration 209000, loss=0.34109678864479065\n",
      "Iteration 210000, loss=0.972896933555603\n",
      "Iteration 211000, loss=0.33238503336906433\n",
      "Iteration 212000, loss=0.20533788204193115\n",
      "Iteration 213000, loss=0.017078233882784843\n",
      "Iteration 214000, loss=0.15598013997077942\n",
      "Iteration 215000, loss=0.35185104608535767\n",
      "Iteration 216000, loss=0.1543450802564621\n",
      "Iteration 217000, loss=0.3445774018764496\n",
      "Iteration 218000, loss=0.31687602400779724\n",
      "Iteration 219000, loss=0.012505387887358665\n",
      "Iteration 220000, loss=0.1707008183002472\n",
      "Iteration 221000, loss=0.29073235392570496\n",
      "Iteration 222000, loss=0.5293721556663513\n",
      "Iteration 223000, loss=0.01902063377201557\n",
      "Iteration 224000, loss=0.33824989199638367\n",
      "Iteration 225000, loss=0.605683445930481\n",
      "Iteration 226000, loss=0.3245197832584381\n",
      "Iteration 227000, loss=1.0\n",
      "Iteration 228000, loss=0.2886030375957489\n",
      "Iteration 229000, loss=0.1982896625995636\n",
      "Iteration 230000, loss=0.22691069543361664\n",
      "Iteration 231000, loss=0.17779912054538727\n",
      "Iteration 232000, loss=0.2794678509235382\n",
      "Iteration 233000, loss=0.3433561325073242\n",
      "Iteration 234000, loss=0.0009520641178824008\n",
      "Iteration 235000, loss=0.18299055099487305\n",
      "Iteration 236000, loss=0.19149711728096008\n",
      "Iteration 237000, loss=0.31587672233581543\n",
      "Iteration 238000, loss=0.2833966016769409\n",
      "Iteration 239000, loss=0.19785988330841064\n",
      "Iteration 240000, loss=0.0022781670559197664\n",
      "Iteration 241000, loss=0.1782657504081726\n",
      "Iteration 242000, loss=0.07108109444379807\n",
      "Iteration 243000, loss=0.1724131554365158\n",
      "Iteration 244000, loss=0.37223950028419495\n",
      "Iteration 245000, loss=0.20350518822669983\n",
      "Iteration 246000, loss=0.26803719997406006\n",
      "Iteration 247000, loss=0.020655682310461998\n",
      "Iteration 248000, loss=0.33953234553337097\n",
      "Iteration 249000, loss=0.17251235246658325\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create some input variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)\n",
    "\n",
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "#similarity = merge([target, context], mode='cos', dot_axes=0)\n",
    "#similarity = Dot(axes = 1, normalize = True)([target, context])\n",
    "\n",
    "# now perform the dot product operation to get a similarity measure\n",
    "#dot_product = merge([target, context], mode='dot', dot_axes=1)\n",
    "dot_product = Dot(axes = 1)([target, context])\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "# create the primary training model\n",
    "model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "    \n",
    "    \n",
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "\n",
    "    if cnt % 1000 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "        losses['tanakh'].append((cnt,loss))\n",
    "#     if cnt % 8 == 0:\n",
    "#         sim_cb.run_sim()\n",
    "\n",
    "model.save_weights(str('./weights/'+'tanakh'+'.h5'))\n",
    "save_obj(losses, 'losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quran Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1740, 796], [86, 569], [1846, 652], [682, 1975], [841, 257], [1975, 1975], [1624, 97], [1743, 1657], [1975, 810], [1812, 1439]] [0, 0, 0, 1, 1, 1, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "#The size of each vocabulary of each text\n",
    "vocab_size = len(quran_encoder)\n",
    "\n",
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = skipgrams(quran_encoded_t, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quran Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss=0.25243324041366577\n",
      "Iteration 1000, loss=0.24677065014839172\n",
      "Iteration 2000, loss=0.24219286441802979\n",
      "Iteration 3000, loss=0.24709264934062958\n",
      "Iteration 4000, loss=0.2373056411743164\n",
      "Iteration 5000, loss=0.2351531833410263\n",
      "Iteration 6000, loss=0.24241654574871063\n",
      "Iteration 7000, loss=0.22974948585033417\n",
      "Iteration 8000, loss=0.24444930255413055\n",
      "Iteration 9000, loss=0.23712977766990662\n",
      "Iteration 10000, loss=0.25728148221969604\n",
      "Iteration 11000, loss=0.25772929191589355\n",
      "Iteration 12000, loss=0.18605613708496094\n",
      "Iteration 13000, loss=0.24840690195560455\n",
      "Iteration 14000, loss=0.25498345494270325\n",
      "Iteration 15000, loss=0.24232202768325806\n",
      "Iteration 16000, loss=0.15729932487010956\n",
      "Iteration 17000, loss=0.3301025331020355\n",
      "Iteration 18000, loss=0.24706728756427765\n",
      "Iteration 19000, loss=0.23765262961387634\n",
      "Iteration 20000, loss=0.2576124966144562\n",
      "Iteration 21000, loss=0.17349809408187866\n",
      "Iteration 22000, loss=0.2487015277147293\n",
      "Iteration 23000, loss=0.24955739080905914\n",
      "Iteration 24000, loss=0.2365877479314804\n",
      "Iteration 25000, loss=0.2591663897037506\n",
      "Iteration 26000, loss=0.24303527176380157\n",
      "Iteration 27000, loss=0.40158945322036743\n",
      "Iteration 28000, loss=0.23150059580802917\n",
      "Iteration 29000, loss=0.28834864497184753\n",
      "Iteration 30000, loss=0.28219807147979736\n",
      "Iteration 31000, loss=0.22261551022529602\n",
      "Iteration 32000, loss=0.26927047967910767\n",
      "Iteration 33000, loss=0.1782989203929901\n",
      "Iteration 34000, loss=0.24167756736278534\n",
      "Iteration 35000, loss=0.22293977439403534\n",
      "Iteration 36000, loss=0.3049260973930359\n",
      "Iteration 37000, loss=0.23315086960792542\n",
      "Iteration 38000, loss=0.28132155537605286\n",
      "Iteration 39000, loss=0.2432987540960312\n",
      "Iteration 40000, loss=0.20561479032039642\n",
      "Iteration 41000, loss=0.2657381594181061\n",
      "Iteration 42000, loss=0.22017891705036163\n",
      "Iteration 43000, loss=0.19405660033226013\n",
      "Iteration 44000, loss=0.290568083524704\n",
      "Iteration 45000, loss=0.21859228610992432\n",
      "Iteration 46000, loss=0.21615734696388245\n",
      "Iteration 47000, loss=0.2598331570625305\n",
      "Iteration 48000, loss=0.22067224979400635\n",
      "Iteration 49000, loss=0.2608264684677124\n",
      "Iteration 50000, loss=0.22634999454021454\n",
      "Iteration 51000, loss=0.2335648387670517\n",
      "Iteration 52000, loss=0.25889676809310913\n",
      "Iteration 53000, loss=0.20549169182777405\n",
      "Iteration 54000, loss=0.1812354177236557\n",
      "Iteration 55000, loss=0.0859498605132103\n",
      "Iteration 56000, loss=0.019979560747742653\n",
      "Iteration 57000, loss=0.29618576169013977\n",
      "Iteration 58000, loss=0.20979054272174835\n",
      "Iteration 59000, loss=0.21549317240715027\n",
      "Iteration 60000, loss=0.19132375717163086\n",
      "Iteration 61000, loss=0.19971109926700592\n",
      "Iteration 62000, loss=0.013135387562215328\n",
      "Iteration 63000, loss=0.14002254605293274\n",
      "Iteration 64000, loss=0.18111465871334076\n",
      "Iteration 65000, loss=0.17170360684394836\n",
      "Iteration 66000, loss=0.18544058501720428\n",
      "Iteration 67000, loss=0.2891986668109894\n",
      "Iteration 68000, loss=0.2906952500343323\n",
      "Iteration 69000, loss=0.20629620552062988\n",
      "Iteration 70000, loss=0.20423109829425812\n",
      "Iteration 71000, loss=0.215873122215271\n",
      "Iteration 72000, loss=0.18859462440013885\n",
      "Iteration 73000, loss=0.21772480010986328\n",
      "Iteration 74000, loss=0.24478943645954132\n",
      "Iteration 75000, loss=0.20043092966079712\n",
      "Iteration 76000, loss=0.21536695957183838\n",
      "Iteration 77000, loss=0.21774518489837646\n",
      "Iteration 78000, loss=0.1967528611421585\n",
      "Iteration 79000, loss=0.1841435432434082\n",
      "Iteration 80000, loss=0.2698565423488617\n",
      "Iteration 81000, loss=0.3379824757575989\n",
      "Iteration 82000, loss=0.21862588822841644\n",
      "Iteration 83000, loss=0.212899312376976\n",
      "Iteration 84000, loss=0.3056672513484955\n",
      "Iteration 85000, loss=0.21506716310977936\n",
      "Iteration 86000, loss=0.29542991518974304\n",
      "Iteration 87000, loss=0.00022999443172011524\n",
      "Iteration 88000, loss=0.2268294394016266\n",
      "Iteration 89000, loss=0.29997846484184265\n",
      "Iteration 90000, loss=0.049558237195014954\n",
      "Iteration 91000, loss=0.19197656214237213\n",
      "Iteration 92000, loss=0.29422006011009216\n",
      "Iteration 93000, loss=0.2512660026550293\n",
      "Iteration 94000, loss=0.2136913686990738\n",
      "Iteration 95000, loss=0.7975674867630005\n",
      "Iteration 96000, loss=0.28914085030555725\n",
      "Iteration 97000, loss=0.42095351219177246\n",
      "Iteration 98000, loss=0.2346857190132141\n",
      "Iteration 99000, loss=0.20469754934310913\n",
      "Iteration 100000, loss=0.1920321136713028\n",
      "Iteration 101000, loss=0.29241859912872314\n",
      "Iteration 102000, loss=0.32240554690361023\n",
      "Iteration 103000, loss=0.30738404393196106\n",
      "Iteration 104000, loss=0.19331112504005432\n",
      "Iteration 105000, loss=0.2378503978252411\n",
      "Iteration 106000, loss=0.007046038750559092\n",
      "Iteration 107000, loss=0.15554215013980865\n",
      "Iteration 108000, loss=0.19479748606681824\n",
      "Iteration 109000, loss=0.1954631805419922\n",
      "Iteration 110000, loss=0.15794120728969574\n",
      "Iteration 111000, loss=0.192109614610672\n",
      "Iteration 112000, loss=0.29597488045692444\n",
      "Iteration 113000, loss=0.16306732594966888\n",
      "Iteration 114000, loss=0.7170005440711975\n",
      "Iteration 115000, loss=0.004981225822120905\n",
      "Iteration 116000, loss=0.19519732892513275\n",
      "Iteration 117000, loss=0.2719120681285858\n",
      "Iteration 118000, loss=0.9292888641357422\n",
      "Iteration 119000, loss=0.20257407426834106\n",
      "Iteration 120000, loss=0.27282461524009705\n",
      "Iteration 121000, loss=0.9041283130645752\n",
      "Iteration 122000, loss=0.2280428409576416\n",
      "Iteration 123000, loss=0.18977011740207672\n",
      "Iteration 124000, loss=0.2021607756614685\n",
      "Iteration 125000, loss=0.1939283311367035\n",
      "Iteration 126000, loss=0.21298947930335999\n",
      "Iteration 127000, loss=0.3433065414428711\n",
      "Iteration 128000, loss=0.1907181292772293\n",
      "Iteration 129000, loss=0.390927255153656\n",
      "Iteration 130000, loss=0.20123286545276642\n",
      "Iteration 131000, loss=0.21497796475887299\n",
      "Iteration 132000, loss=0.19430463016033173\n",
      "Iteration 133000, loss=0.16828611493110657\n",
      "Iteration 134000, loss=0.00575080094859004\n",
      "Iteration 135000, loss=0.4133062958717346\n",
      "Iteration 136000, loss=0.19350197911262512\n",
      "Iteration 137000, loss=0.30259785056114197\n",
      "Iteration 138000, loss=0.19737356901168823\n",
      "Iteration 139000, loss=0.055619534105062485\n",
      "Iteration 140000, loss=0.29672572016716003\n",
      "Iteration 141000, loss=0.36136946082115173\n",
      "Iteration 142000, loss=0.00012514401169028133\n",
      "Iteration 143000, loss=0.21139639616012573\n",
      "Iteration 144000, loss=0.4958859384059906\n",
      "Iteration 145000, loss=0.16683213412761688\n",
      "Iteration 146000, loss=0.23881667852401733\n",
      "Iteration 147000, loss=0.049898236989974976\n",
      "Iteration 148000, loss=0.0013216614024713635\n",
      "Iteration 149000, loss=0.20954249799251556\n",
      "Iteration 150000, loss=0.11522367596626282\n",
      "Iteration 151000, loss=0.13984520733356476\n",
      "Iteration 152000, loss=0.1992647498846054\n",
      "Iteration 153000, loss=0.0029533542692661285\n",
      "Iteration 154000, loss=0.19751709699630737\n",
      "Iteration 155000, loss=0.011907150968909264\n",
      "Iteration 156000, loss=0.21483276784420013\n",
      "Iteration 157000, loss=0.2146068662405014\n",
      "Iteration 158000, loss=0.1834714412689209\n",
      "Iteration 159000, loss=0.37339508533477783\n",
      "Iteration 160000, loss=0.18746301531791687\n",
      "Iteration 161000, loss=0.28657597303390503\n",
      "Iteration 162000, loss=0.3362710475921631\n",
      "Iteration 163000, loss=0.054807014763355255\n",
      "Iteration 164000, loss=0.16309674084186554\n",
      "Iteration 165000, loss=0.32263094186782837\n",
      "Iteration 166000, loss=0.17568039894104004\n",
      "Iteration 167000, loss=0.2823197841644287\n",
      "Iteration 168000, loss=0.010460300371050835\n",
      "Iteration 169000, loss=0.7485347986221313\n",
      "Iteration 170000, loss=0.3925148844718933\n",
      "Iteration 171000, loss=0.14849169552326202\n",
      "Iteration 172000, loss=0.1756887137889862\n",
      "Iteration 173000, loss=0.007081767078489065\n",
      "Iteration 174000, loss=0.27268218994140625\n",
      "Iteration 175000, loss=0.1630343347787857\n",
      "Iteration 176000, loss=0.3753717541694641\n",
      "Iteration 177000, loss=0.9999821186065674\n",
      "Iteration 178000, loss=0.2608124017715454\n",
      "Iteration 179000, loss=0.20773644745349884\n",
      "Iteration 180000, loss=0.19971665740013123\n",
      "Iteration 181000, loss=0.16722092032432556\n",
      "Iteration 182000, loss=0.16705560684204102\n",
      "Iteration 183000, loss=0.21958540380001068\n",
      "Iteration 184000, loss=0.1572895050048828\n",
      "Iteration 185000, loss=0.30972179770469666\n",
      "Iteration 186000, loss=0.11286533623933792\n",
      "Iteration 187000, loss=0.18414196372032166\n",
      "Iteration 188000, loss=0.23921844363212585\n",
      "Iteration 189000, loss=0.20904338359832764\n",
      "Iteration 190000, loss=0.6772788763046265\n",
      "Iteration 191000, loss=0.3295660614967346\n",
      "Iteration 192000, loss=0.18794786930084229\n",
      "Iteration 193000, loss=0.30737605690956116\n",
      "Iteration 194000, loss=0.14812247455120087\n",
      "Iteration 195000, loss=0.27826082706451416\n",
      "Iteration 196000, loss=0.1611909419298172\n",
      "Iteration 197000, loss=0.46767503023147583\n",
      "Iteration 198000, loss=0.3408143222332001\n",
      "Iteration 199000, loss=0.17613020539283752\n",
      "Iteration 200000, loss=0.17941121757030487\n",
      "Iteration 201000, loss=0.26227250695228577\n",
      "Iteration 202000, loss=0.2294221818447113\n",
      "Iteration 203000, loss=0.1881365031003952\n",
      "Iteration 204000, loss=0.3176983892917633\n",
      "Iteration 205000, loss=0.333924800157547\n",
      "Iteration 206000, loss=0.12892210483551025\n",
      "Iteration 207000, loss=0.18290303647518158\n",
      "Iteration 208000, loss=0.10387547314167023\n",
      "Iteration 209000, loss=0.3700748383998871\n",
      "Iteration 210000, loss=0.24862827360630035\n",
      "Iteration 211000, loss=0.3266415596008301\n",
      "Iteration 212000, loss=0.9574178457260132\n",
      "Iteration 213000, loss=0.35054558515548706\n",
      "Iteration 214000, loss=0.389238178730011\n",
      "Iteration 215000, loss=0.34139156341552734\n",
      "Iteration 216000, loss=0.18411244451999664\n",
      "Iteration 217000, loss=0.2256077229976654\n",
      "Iteration 218000, loss=0.16119389235973358\n",
      "Iteration 219000, loss=0.1692480593919754\n",
      "Iteration 220000, loss=0.1584608405828476\n",
      "Iteration 221000, loss=0.33787277340888977\n",
      "Iteration 222000, loss=0.025276491418480873\n",
      "Iteration 223000, loss=0.9156870245933533\n",
      "Iteration 224000, loss=0.35004889965057373\n",
      "Iteration 225000, loss=0.43309271335601807\n",
      "Iteration 226000, loss=0.45542603731155396\n",
      "Iteration 227000, loss=0.17752595245838165\n",
      "Iteration 228000, loss=0.2156723141670227\n",
      "Iteration 229000, loss=0.6927266716957092\n",
      "Iteration 230000, loss=0.11888693273067474\n",
      "Iteration 231000, loss=0.2162187099456787\n",
      "Iteration 232000, loss=0.00014569210179615766\n",
      "Iteration 233000, loss=0.1894591599702835\n",
      "Iteration 234000, loss=0.8479519486427307\n",
      "Iteration 235000, loss=0.3034467399120331\n",
      "Iteration 236000, loss=0.3597915470600128\n",
      "Iteration 237000, loss=0.267964243888855\n",
      "Iteration 238000, loss=0.020173652097582817\n",
      "Iteration 239000, loss=0.1878577023744583\n",
      "Iteration 240000, loss=0.3021302819252014\n",
      "Iteration 241000, loss=0.9711064696311951\n",
      "Iteration 242000, loss=0.05545353889465332\n",
      "Iteration 243000, loss=0.3203411400318146\n",
      "Iteration 244000, loss=0.029262954369187355\n",
      "Iteration 245000, loss=0.30364593863487244\n",
      "Iteration 246000, loss=0.17741160094738007\n",
      "Iteration 247000, loss=0.18760396540164948\n",
      "Iteration 248000, loss=0.11456725746393204\n",
      "Iteration 249000, loss=0.09692656993865967\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create some input variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)\n",
    "\n",
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "#similarity = merge([target, context], mode='cos', dot_axes=0)\n",
    "#similarity = Dot(axes = 1, normalize = True)([target, context])\n",
    "\n",
    "# now perform the dot product operation to get a similarity measure\n",
    "#dot_product = merge([target, context], mode='dot', dot_axes=1)\n",
    "dot_product = Dot(axes = 1)([target, context])\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "# create the primary training model\n",
    "model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "    \n",
    "    \n",
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "\n",
    "    if cnt % 1000 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "        losses['quran'].append((cnt,loss))\n",
    "#     if cnt % 8 == 0:\n",
    "#         sim_cb.run_sim()\n",
    "\n",
    "model.save_weights(str('./weights/'+'quran'+'.h5'))\n",
    "save_obj(losses, 'losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1981, 150)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-0.008004</td>\n",
       "      <td>-0.017508</td>\n",
       "      <td>0.019305</td>\n",
       "      <td>-0.035294</td>\n",
       "      <td>-0.013208</td>\n",
       "      <td>-0.018031</td>\n",
       "      <td>-0.009425</td>\n",
       "      <td>0.042773</td>\n",
       "      <td>0.004820</td>\n",
       "      <td>0.031941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022435</td>\n",
       "      <td>0.028444</td>\n",
       "      <td>0.041003</td>\n",
       "      <td>0.004340</td>\n",
       "      <td>0.012442</td>\n",
       "      <td>0.022538</td>\n",
       "      <td>0.020571</td>\n",
       "      <td>-0.030228</td>\n",
       "      <td>0.009786</td>\n",
       "      <td>-0.035771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aad</th>\n",
       "      <td>-0.043280</td>\n",
       "      <td>0.020593</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>-0.045096</td>\n",
       "      <td>-0.065076</td>\n",
       "      <td>-0.021197</td>\n",
       "      <td>0.042265</td>\n",
       "      <td>-0.001318</td>\n",
       "      <td>-0.115049</td>\n",
       "      <td>-0.039032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050158</td>\n",
       "      <td>-0.048636</td>\n",
       "      <td>-0.046291</td>\n",
       "      <td>-0.020075</td>\n",
       "      <td>-0.047856</td>\n",
       "      <td>0.101097</td>\n",
       "      <td>0.045071</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.056095</td>\n",
       "      <td>-0.004092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaron</th>\n",
       "      <td>0.055953</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>-0.016078</td>\n",
       "      <td>0.047872</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.016177</td>\n",
       "      <td>-0.039026</td>\n",
       "      <td>-0.007048</td>\n",
       "      <td>0.035373</td>\n",
       "      <td>-0.083134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046544</td>\n",
       "      <td>-0.012071</td>\n",
       "      <td>-0.036066</td>\n",
       "      <td>-0.011777</td>\n",
       "      <td>-0.006468</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.018141</td>\n",
       "      <td>0.006878</td>\n",
       "      <td>-0.088374</td>\n",
       "      <td>0.017967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.045857</td>\n",
       "      <td>0.012578</td>\n",
       "      <td>-0.020895</td>\n",
       "      <td>-0.004270</td>\n",
       "      <td>0.071471</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>0.015572</td>\n",
       "      <td>0.045639</td>\n",
       "      <td>0.002846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036808</td>\n",
       "      <td>-0.005546</td>\n",
       "      <td>0.017284</td>\n",
       "      <td>0.040434</td>\n",
       "      <td>0.055834</td>\n",
       "      <td>0.039540</td>\n",
       "      <td>-0.019949</td>\n",
       "      <td>-0.044315</td>\n",
       "      <td>0.045188</td>\n",
       "      <td>-0.035941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abide</th>\n",
       "      <td>-0.049343</td>\n",
       "      <td>-0.001358</td>\n",
       "      <td>-0.013341</td>\n",
       "      <td>-0.079927</td>\n",
       "      <td>-0.002295</td>\n",
       "      <td>-0.036621</td>\n",
       "      <td>-0.010116</td>\n",
       "      <td>0.024246</td>\n",
       "      <td>-0.046053</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048401</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>-0.084583</td>\n",
       "      <td>0.015470</td>\n",
       "      <td>-0.013865</td>\n",
       "      <td>0.018491</td>\n",
       "      <td>0.050974</td>\n",
       "      <td>0.062327</td>\n",
       "      <td>0.021287</td>\n",
       "      <td>-0.010545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6    \\\n",
       "a       -0.008004 -0.017508  0.019305 -0.035294 -0.013208 -0.018031 -0.009425   \n",
       "aad     -0.043280  0.020593  0.006402 -0.045096 -0.065076 -0.021197  0.042265   \n",
       "aaron    0.055953  0.014981 -0.016078  0.047872  0.000456  0.016177 -0.039026   \n",
       "abandon  0.001636  0.045857  0.012578 -0.020895 -0.004270  0.071471 -0.018808   \n",
       "abide   -0.049343 -0.001358 -0.013341 -0.079927 -0.002295 -0.036621 -0.010116   \n",
       "\n",
       "              7         8         9    ...       140       141       142  \\\n",
       "a        0.042773  0.004820  0.031941  ...  0.022435  0.028444  0.041003   \n",
       "aad     -0.001318 -0.115049 -0.039032  ... -0.050158 -0.048636 -0.046291   \n",
       "aaron   -0.007048  0.035373 -0.083134  ...  0.046544 -0.012071 -0.036066   \n",
       "abandon  0.015572  0.045639  0.002846  ...  0.036808 -0.005546  0.017284   \n",
       "abide    0.024246 -0.046053  0.011096  ... -0.048401  0.001708 -0.084583   \n",
       "\n",
       "              143       144       145       146       147       148       149  \n",
       "a        0.004340  0.012442  0.022538  0.020571 -0.030228  0.009786 -0.035771  \n",
       "aad     -0.020075 -0.047856  0.101097  0.045071  0.001910  0.056095 -0.004092  \n",
       "aaron   -0.011777 -0.006468  0.001492  0.018141  0.006878 -0.088374  0.017967  \n",
       "abandon  0.040434  0.055834  0.039540 -0.019949 -0.044315  0.045188 -0.035941  \n",
       "abide    0.015470 -0.013865  0.018491  0.050974  0.062327  0.021287 -0.010545  \n",
       "\n",
       "[5 rows x 150 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.layers[2].get_weights()[0][:]\n",
    "\n",
    "print(weights.shape)\n",
    "pd.DataFrame(weights, index=quran_decoder.values()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1981, 1981)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'good': ['attention', 'obvious', 'believers', 'forgiving', 'fairly'],\n",
       " 'love': ['kindness', 'lesson', 'midst', 'delight', 'deed'],\n",
       " 'creator': ['dwell', 'clement', 'encounter', 'struck', 'demeaning'],\n",
       " 'evil': ['hospitality', 'severe', 'during', 'joseph', 'mislead']}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "similar_words = {search_term: [quran_decoder[idx] for idx in distance_matrix[quran_encoder[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['good', 'love', 'creator', 'evil']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sutras': [(0, 0.24973024427890778),\n",
       "  (1000, 0.25718575716018677),\n",
       "  (2000, 0.25558632612228394),\n",
       "  (3000, 0.2433577924966812),\n",
       "  (4000, 0.260622501373291),\n",
       "  (5000, 0.25719791650772095),\n",
       "  (6000, 0.23097988963127136),\n",
       "  (7000, 0.20916429162025452),\n",
       "  (8000, 0.3073486089706421),\n",
       "  (9000, 0.19192072749137878),\n",
       "  (10000, 0.19433946907520294),\n",
       "  (11000, 0.18392619490623474),\n",
       "  (12000, 6.10245388088515e-06),\n",
       "  (13000, 0.07408968359231949),\n",
       "  (14000, 0.15859222412109375),\n",
       "  (15000, 0.0),\n",
       "  (16000, 0.14458470046520233),\n",
       "  (17000, 0.16416716575622559),\n",
       "  (18000, 0.0),\n",
       "  (19000, 0.14185231924057007),\n",
       "  (20000, 0.40220674872398376),\n",
       "  (21000, 1.080570655176416e-08),\n",
       "  (22000, 0.12044154107570648),\n",
       "  (23000, 0.12080275267362595),\n",
       "  (24000, 0.4184947907924652),\n",
       "  (25000, 0.1622585654258728),\n",
       "  (26000, 0.11619251221418381),\n",
       "  (27000, 0.12574687600135803),\n",
       "  (28000, 0.5550995469093323),\n",
       "  (29000, 0.35841104388237),\n",
       "  (30000, 0.037461139261722565),\n",
       "  (31000, 0.09715898334980011),\n",
       "  (32000, 0.01581178605556488),\n",
       "  (33000, 8.881784197001252e-12),\n",
       "  (34000, 0.3180404007434845),\n",
       "  (35000, 6.87805368215777e-10),\n",
       "  (36000, 0.006610352545976639),\n",
       "  (37000, 0.0),\n",
       "  (38000, 0.10130457580089569),\n",
       "  (39000, 8.100542459033022e-09),\n",
       "  (40000, 0.11527970433235168),\n",
       "  (41000, 0.39621657133102417),\n",
       "  (42000, 0.15367254614830017),\n",
       "  (43000, 0.05906544253230095),\n",
       "  (44000, 0.0),\n",
       "  (45000, 0.1802133470773697),\n",
       "  (46000, 0.8652156591415405),\n",
       "  (47000, 0.0),\n",
       "  (48000, 0.07311829924583435),\n",
       "  (49000, 0.4738459289073944),\n",
       "  (50000, 0.8998289704322815),\n",
       "  (51000, 0.07531290501356125),\n",
       "  (52000, 0.17321771383285522),\n",
       "  (53000, 0.017206234857439995),\n",
       "  (54000, 0.3862016797065735),\n",
       "  (55000, 0.081679567694664),\n",
       "  (56000, 0.034255221486091614),\n",
       "  (57000, 0.03946111723780632),\n",
       "  (58000, 0.027867183089256287),\n",
       "  (59000, 0.6362723112106323),\n",
       "  (60000, 0.014194083400070667),\n",
       "  (61000, 0.0013977487105876207),\n",
       "  (62000, 0.45260852575302124),\n",
       "  (63000, 0.01728617027401924),\n",
       "  (64000, 1.0),\n",
       "  (65000, 1.2054229614477663e-08),\n",
       "  (66000, 0.06739740073680878),\n",
       "  (67000, 0.05135498568415642),\n",
       "  (68000, 0.0),\n",
       "  (69000, 0.3541715145111084),\n",
       "  (70000, 6.052214871488104e-07),\n",
       "  (71000, 0.0002941424318123609),\n",
       "  (72000, 1.727352719171904e-05),\n",
       "  (73000, 0.0748562291264534),\n",
       "  (74000, 0.21678459644317627),\n",
       "  (75000, 0.3979043662548065),\n",
       "  (76000, 0.0),\n",
       "  (77000, 0.01871674507856369),\n",
       "  (78000, 0.04822547733783722),\n",
       "  (79000, 0.05538320541381836),\n",
       "  (80000, 0.0024130402598530054),\n",
       "  (81000, 6.87805368215777e-12),\n",
       "  (82000, 0.003936116583645344),\n",
       "  (83000, 0.8376281261444092),\n",
       "  (84000, 0.5479239821434021),\n",
       "  (85000, 0.9238711595535278),\n",
       "  (86000, 0.0),\n",
       "  (87000, 0.003793171839788556),\n",
       "  (88000, 0.4661555290222168),\n",
       "  (89000, 0.9632187485694885),\n",
       "  (90000, 0.46534350514411926),\n",
       "  (91000, 0.0697835311293602),\n",
       "  (92000, 0.5853704214096069),\n",
       "  (93000, 3.958008107929345e-07),\n",
       "  (94000, 0.05657279118895531),\n",
       "  (95000, 0.6460508108139038),\n",
       "  (96000, 0.06413009762763977),\n",
       "  (97000, 0.3198619484901428),\n",
       "  (98000, 0.000690442742779851),\n",
       "  (99000, 0.00015875513781793416),\n",
       "  (100000, 0.544693648815155),\n",
       "  (101000, 3.0375564165296964e-05),\n",
       "  (102000, 0.03650357201695442),\n",
       "  (103000, 0.004440499935299158),\n",
       "  (104000, 0.6886754035949707),\n",
       "  (105000, 0.005987442098557949),\n",
       "  (106000, 0.0),\n",
       "  (107000, 0.0),\n",
       "  (108000, 2.6487718969292473e-06),\n",
       "  (109000, 0.00021915858087595552),\n",
       "  (110000, 0.02995545230805874),\n",
       "  (111000, 0.043031856417655945),\n",
       "  (112000, 0.5765392780303955),\n",
       "  (113000, 0.0504111647605896),\n",
       "  (114000, 0.0),\n",
       "  (115000, 0.6922189593315125),\n",
       "  (116000, 0.010550014674663544),\n",
       "  (117000, 1.1951328815484885e-11),\n",
       "  (118000, 0.06551288813352585),\n",
       "  (119000, 0.0523541234433651),\n",
       "  (120000, 0.0),\n",
       "  (121000, 0.7387518286705017),\n",
       "  (122000, 4.921131676383084e-06),\n",
       "  (123000, 0.06463424116373062),\n",
       "  (124000, 0.04844653606414795),\n",
       "  (125000, 0.018876483663916588),\n",
       "  (126000, 0.0),\n",
       "  (127000, 0.01708044297993183),\n",
       "  (128000, 0.027186905965209007),\n",
       "  (129000, 5.609555955743417e-06),\n",
       "  (130000, 0.03135916590690613),\n",
       "  (131000, 0.0009219347848556936),\n",
       "  (132000, 0.0),\n",
       "  (133000, 0.0),\n",
       "  (134000, 0.03845598176121712),\n",
       "  (135000, 0.0051117390394210815),\n",
       "  (136000, 0.7790315747261047),\n",
       "  (137000, 0.8883661031723022),\n",
       "  (138000, 0.09612724184989929),\n",
       "  (139000, 0.0),\n",
       "  (140000, 0.07824288308620453),\n",
       "  (141000, 6.40117036709853e-07),\n",
       "  (142000, 0.07494430243968964),\n",
       "  (143000, 0.0006076015415601432),\n",
       "  (144000, 1.5076295767357806e-10),\n",
       "  (145000, 0.057480588555336),\n",
       "  (146000, 6.549415411427617e-05),\n",
       "  (147000, 0.0),\n",
       "  (148000, 0.011073734611272812),\n",
       "  (149000, 0.04819644242525101),\n",
       "  (150000, 0.005200262181460857),\n",
       "  (151000, 0.8654420971870422),\n",
       "  (152000, 0.5244468450546265),\n",
       "  (153000, 0.1042083129286766),\n",
       "  (154000, 0.6589615941047668),\n",
       "  (155000, 0.06086339056491852),\n",
       "  (156000, 1.0027179087046534e-10),\n",
       "  (157000, 0.023727308958768845),\n",
       "  (158000, 0.036215998232364655),\n",
       "  (159000, 0.4501707851886749),\n",
       "  (160000, 1.1844419134376949e-07),\n",
       "  (161000, 0.9190422892570496),\n",
       "  (162000, 3.16796359811633e-07),\n",
       "  (163000, 0.174782395362854),\n",
       "  (164000, 0.0),\n",
       "  (165000, 2.049456043096143e-06),\n",
       "  (166000, 0.0012873602099716663),\n",
       "  (167000, 0.0506853386759758),\n",
       "  (168000, 0.18794894218444824),\n",
       "  (169000, 0.042610589414834976),\n",
       "  (170000, 0.9696603417396545),\n",
       "  (171000, 0.06573965400457382),\n",
       "  (172000, 0.0),\n",
       "  (173000, 0.004598905332386494),\n",
       "  (174000, 0.0),\n",
       "  (175000, 0.2652890980243683),\n",
       "  (176000, 7.17188015642023e-08),\n",
       "  (177000, 0.0),\n",
       "  (178000, 0.31453511118888855),\n",
       "  (179000, 5.115907697472721e-13),\n",
       "  (180000, 0.014478757977485657),\n",
       "  (181000, 1.4959425698179984e-08),\n",
       "  (182000, 0.00027983609470538795),\n",
       "  (183000, 5.280580080579966e-05),\n",
       "  (184000, 0.5010387897491455),\n",
       "  (185000, 0.010416803881525993),\n",
       "  (186000, 0.08294756710529327),\n",
       "  (187000, 0.051229942589998245),\n",
       "  (188000, 0.07244323939085007),\n",
       "  (189000, 0.00048577532288618386),\n",
       "  (190000, 0.19799655675888062),\n",
       "  (191000, 2.6521604013396427e-05),\n",
       "  (192000, 0.6287352442741394),\n",
       "  (193000, 0.16346803307533264),\n",
       "  (194000, 0.07386720925569534),\n",
       "  (195000, 0.0038785662036389112),\n",
       "  (196000, 0.003976977895945311),\n",
       "  (197000, 0.9803812503814697),\n",
       "  (198000, 3.212402225472033e-05),\n",
       "  (199000, 2.929084530478576e-09),\n",
       "  (200000, 0.0),\n",
       "  (201000, 0.9262284636497498),\n",
       "  (202000, 0.0164601169526577),\n",
       "  (203000, 0.07709026336669922),\n",
       "  (204000, 0.021100923418998718),\n",
       "  (205000, 0.24655038118362427),\n",
       "  (206000, 0.004656767938286066),\n",
       "  (207000, 0.013949098996818066),\n",
       "  (208000, 0.0),\n",
       "  (209000, 0.0),\n",
       "  (210000, 0.010214419104158878),\n",
       "  (211000, 0.18204854428768158),\n",
       "  (212000, 0.01883046329021454),\n",
       "  (213000, 0.9999399185180664),\n",
       "  (214000, 0.00475451722741127),\n",
       "  (215000, 1.035971308738226e-11),\n",
       "  (216000, 0.0),\n",
       "  (217000, 2.2737367544323206e-13),\n",
       "  (218000, 0.10862576961517334),\n",
       "  (219000, 0.0006489910301752388),\n",
       "  (220000, 0.0009480626904405653),\n",
       "  (221000, 0.07567255944013596),\n",
       "  (222000, 0.005074269138276577),\n",
       "  (223000, 0.004345814231783152),\n",
       "  (224000, 0.054631344974040985),\n",
       "  (225000, 0.01617990992963314),\n",
       "  (226000, 0.0014583816519007087),\n",
       "  (227000, 0.9999995231628418),\n",
       "  (228000, 0.0),\n",
       "  (229000, 0.00534770218655467),\n",
       "  (230000, 1.0436593811391504e-07),\n",
       "  (231000, 0.4231722056865692),\n",
       "  (232000, 0.0),\n",
       "  (233000, 0.01999419368803501),\n",
       "  (234000, 0.07988834381103516),\n",
       "  (235000, 0.0),\n",
       "  (236000, 0.0),\n",
       "  (237000, 0.07572958618402481),\n",
       "  (238000, 0.0012209041742607951),\n",
       "  (239000, 0.0),\n",
       "  (240000, 0.0),\n",
       "  (241000, 0.06590519100427628),\n",
       "  (242000, 0.5992646217346191),\n",
       "  (243000, 0.29150643944740295),\n",
       "  (244000, 0.03764623776078224),\n",
       "  (245000, 0.06486539542675018),\n",
       "  (246000, 0.0018302288372069597),\n",
       "  (247000, 0.07797356694936752),\n",
       "  (248000, 0.0),\n",
       "  (249000, 0.00016318853886332363)],\n",
       " 'vedas': [(0, 0.24736115336418152),\n",
       "  (1000, 0.24113285541534424),\n",
       "  (2000, 0.24639952182769775),\n",
       "  (3000, 0.2547740936279297),\n",
       "  (4000, 0.26288464665412903),\n",
       "  (5000, 0.2423153668642044),\n",
       "  (6000, 0.2388557493686676),\n",
       "  (7000, 0.2458762377500534),\n",
       "  (8000, 0.2385142296552658),\n",
       "  (9000, 0.2505006790161133),\n",
       "  (10000, 0.2453857958316803),\n",
       "  (11000, 0.2425558716058731),\n",
       "  (12000, 0.25083354115486145),\n",
       "  (13000, 0.24563181400299072),\n",
       "  (14000, 0.2528590261936188),\n",
       "  (15000, 0.2404453158378601),\n",
       "  (16000, 0.25183531641960144),\n",
       "  (17000, 0.25450819730758667),\n",
       "  (18000, 0.2967410981655121),\n",
       "  (19000, 0.22757062315940857),\n",
       "  (20000, 0.2662801444530487),\n",
       "  (21000, 0.23045112192630768),\n",
       "  (22000, 0.2670726776123047),\n",
       "  (23000, 0.2222745418548584),\n",
       "  (24000, 0.27935120463371277),\n",
       "  (25000, 0.22056269645690918),\n",
       "  (26000, 0.23376606404781342),\n",
       "  (27000, 0.23811936378479004),\n",
       "  (28000, 0.2527710795402527),\n",
       "  (29000, 0.28323566913604736),\n",
       "  (30000, 0.25335267186164856),\n",
       "  (31000, 0.24500083923339844),\n",
       "  (32000, 0.2603400647640228),\n",
       "  (33000, 0.2320081889629364),\n",
       "  (34000, 0.22927144169807434),\n",
       "  (35000, 0.2658301591873169),\n",
       "  (36000, 0.23615020513534546),\n",
       "  (37000, 0.23181810975074768),\n",
       "  (38000, 0.001806991407647729),\n",
       "  (39000, 0.23875615000724792),\n",
       "  (40000, 0.26277557015419006),\n",
       "  (41000, 0.17532220482826233),\n",
       "  (42000, 0.2508780360221863),\n",
       "  (43000, 0.37590089440345764),\n",
       "  (44000, 0.4176936149597168),\n",
       "  (45000, 0.25827449560165405),\n",
       "  (46000, 0.24358654022216797),\n",
       "  (47000, 0.28526899218559265),\n",
       "  (48000, 0.002809313125908375),\n",
       "  (49000, 0.2314806878566742),\n",
       "  (50000, 0.010370466858148575),\n",
       "  (51000, 0.2367561310529709),\n",
       "  (52000, 0.22543565928936005),\n",
       "  (53000, 1.0),\n",
       "  (54000, 0.22944539785385132),\n",
       "  (55000, 0.24399861693382263),\n",
       "  (56000, 0.22842144966125488),\n",
       "  (57000, 0.2823619842529297),\n",
       "  (58000, 0.26161596179008484),\n",
       "  (59000, 0.21230782568454742),\n",
       "  (60000, 0.2547789514064789),\n",
       "  (61000, 0.2921917140483856),\n",
       "  (62000, 0.21279968321323395),\n",
       "  (63000, 0.2307061403989792),\n",
       "  (64000, 0.2530316412448883),\n",
       "  (65000, 0.2662244141101837),\n",
       "  (66000, 0.20508477091789246),\n",
       "  (67000, 1.0),\n",
       "  (68000, 0.22421908378601074),\n",
       "  (69000, 0.24189035594463348),\n",
       "  (70000, 0.2693307399749756),\n",
       "  (71000, 0.26856061816215515),\n",
       "  (72000, 0.22131706774234772),\n",
       "  (73000, 0.2862584590911865),\n",
       "  (74000, 0.17929445207118988),\n",
       "  (75000, 0.24193987250328064),\n",
       "  (76000, 0.272990345954895),\n",
       "  (77000, 0.25156939029693604),\n",
       "  (78000, 0.2583739161491394),\n",
       "  (79000, 0.2531167268753052),\n",
       "  (80000, 0.20359773933887482),\n",
       "  (81000, 0.22287024557590485),\n",
       "  (82000, 0.23840239644050598),\n",
       "  (83000, 0.32982075214385986),\n",
       "  (84000, 0.2394210696220398),\n",
       "  (85000, 0.2746475040912628),\n",
       "  (86000, 0.23914434015750885),\n",
       "  (87000, 0.27487221360206604),\n",
       "  (88000, 0.27471187710762024),\n",
       "  (89000, 0.00034107224200852215),\n",
       "  (90000, 0.38861215114593506),\n",
       "  (91000, 0.22327032685279846),\n",
       "  (92000, 0.26639026403427124),\n",
       "  (93000, 0.2144092321395874),\n",
       "  (94000, 0.22851726412773132),\n",
       "  (95000, 0.22999528050422668),\n",
       "  (96000, 0.0008316703024320304),\n",
       "  (97000, 0.4131588637828827),\n",
       "  (98000, 0.18168041110038757),\n",
       "  (99000, 0.23080940544605255),\n",
       "  (100000, 0.25206178426742554),\n",
       "  (101000, 0.22092896699905396),\n",
       "  (102000, 0.22703774273395538),\n",
       "  (103000, 0.27721622586250305),\n",
       "  (104000, 0.23698504269123077),\n",
       "  (105000, 0.09186951071023941),\n",
       "  (106000, 0.2729674279689789),\n",
       "  (107000, 0.2109147608280182),\n",
       "  (108000, 0.36845001578330994),\n",
       "  (109000, 0.23683680593967438),\n",
       "  (110000, 0.004237941931933165),\n",
       "  (111000, 0.13575556874275208),\n",
       "  (112000, 0.3121618926525116),\n",
       "  (113000, 0.4487687349319458),\n",
       "  (114000, 0.279451847076416),\n",
       "  (115000, 0.2850753366947174),\n",
       "  (116000, 0.248897522687912),\n",
       "  (117000, 0.08827246725559235),\n",
       "  (118000, 0.19926483929157257),\n",
       "  (119000, 0.27683693170547485),\n",
       "  (120000, 0.4442500174045563),\n",
       "  (121000, 0.2071167677640915),\n",
       "  (122000, 0.294452965259552),\n",
       "  (123000, 0.2728986144065857),\n",
       "  (124000, 0.20418721437454224),\n",
       "  (125000, 0.26718759536743164),\n",
       "  (126000, 0.11929295212030411),\n",
       "  (127000, 0.1711375117301941),\n",
       "  (128000, 0.24060191214084625),\n",
       "  (129000, 0.23165689408779144),\n",
       "  (130000, 0.20641997456550598),\n",
       "  (131000, 0.14994384348392487),\n",
       "  (132000, 0.2777470350265503),\n",
       "  (133000, 0.34690287709236145),\n",
       "  (134000, 0.2090134173631668),\n",
       "  (135000, 0.21222145855426788),\n",
       "  (136000, 0.21224896609783173),\n",
       "  (137000, 0.30422571301460266),\n",
       "  (138000, 0.26550784707069397),\n",
       "  (139000, 0.19127815961837769),\n",
       "  (140000, 0.2160515934228897),\n",
       "  (141000, 0.2017529010772705),\n",
       "  (142000, 0.3220435380935669),\n",
       "  (143000, 0.29423215985298157),\n",
       "  (144000, 0.2867545485496521),\n",
       "  (145000, 0.21554028987884521),\n",
       "  (146000, 0.30036407709121704),\n",
       "  (147000, 0.21364492177963257),\n",
       "  (148000, 0.2396842986345291),\n",
       "  (149000, 0.0006808885373175144),\n",
       "  (150000, 0.22160831093788147),\n",
       "  (151000, 0.22217679023742676),\n",
       "  (152000, 0.2131805568933487),\n",
       "  (153000, 0.2953701615333557),\n",
       "  (154000, 0.1776116043329239),\n",
       "  (155000, 0.6967933177947998),\n",
       "  (156000, 0.0006745359278284013),\n",
       "  (157000, 0.19873164594173431),\n",
       "  (158000, 0.1949472874403),\n",
       "  (159000, 0.3145427107810974),\n",
       "  (160000, 0.25786784291267395),\n",
       "  (161000, 0.2096894085407257),\n",
       "  (162000, 0.30972060561180115),\n",
       "  (163000, 0.3107244372367859),\n",
       "  (164000, 0.20280230045318604),\n",
       "  (165000, 0.294836163520813),\n",
       "  (166000, 0.19182088971138),\n",
       "  (167000, 0.28085851669311523),\n",
       "  (168000, 0.30271995067596436),\n",
       "  (169000, 0.20775361359119415),\n",
       "  (170000, 0.2977795898914337),\n",
       "  (171000, 0.014558305032551289),\n",
       "  (172000, 0.36674052476882935),\n",
       "  (173000, 0.20642200112342834),\n",
       "  (174000, 0.00014300808834377676),\n",
       "  (175000, 0.21026542782783508),\n",
       "  (176000, 0.22571825981140137),\n",
       "  (177000, 0.3172484338283539),\n",
       "  (178000, 0.13236019015312195),\n",
       "  (179000, 0.21862725913524628),\n",
       "  (180000, 0.4252612590789795),\n",
       "  (181000, 0.21656078100204468),\n",
       "  (182000, 0.22365374863147736),\n",
       "  (183000, 0.25937458872795105),\n",
       "  (184000, 0.18540915846824646),\n",
       "  (185000, 0.20855994522571564),\n",
       "  (186000, 0.22838416695594788),\n",
       "  (187000, 0.20293410122394562),\n",
       "  (188000, 0.3146146535873413),\n",
       "  (189000, 0.26233577728271484),\n",
       "  (190000, 0.29273635149002075),\n",
       "  (191000, 0.296581894159317),\n",
       "  (192000, 0.2995847761631012),\n",
       "  (193000, 0.29131004214286804),\n",
       "  (194000, 1.0),\n",
       "  (195000, 0.11272469162940979),\n",
       "  (196000, 0.004286903887987137),\n",
       "  (197000, 0.47285303473472595),\n",
       "  (198000, 0.09090589731931686),\n",
       "  (199000, 0.21822160482406616),\n",
       "  (200000, 0.3202449381351471),\n",
       "  (201000, 0.21954655647277832),\n",
       "  (202000, 0.13576360046863556),\n",
       "  (203000, 0.2854481637477875),\n",
       "  (204000, 0.29764261841773987),\n",
       "  (205000, 0.2891651391983032),\n",
       "  (206000, 0.25404563546180725),\n",
       "  (207000, 0.01189674623310566),\n",
       "  (208000, 0.36741626262664795),\n",
       "  (209000, 0.3852654695510864),\n",
       "  (210000, 0.0012825733283534646),\n",
       "  (211000, 0.0018109765369445086),\n",
       "  (212000, 0.21935038268566132),\n",
       "  (213000, 0.2963404357433319),\n",
       "  (214000, 0.6459922790527344),\n",
       "  (215000, 0.28868794441223145),\n",
       "  (216000, 0.006386052817106247),\n",
       "  (217000, 0.21221885085105896),\n",
       "  (218000, 0.2883974313735962),\n",
       "  (219000, 0.2960782051086426),\n",
       "  (220000, 0.3458397686481476),\n",
       "  (221000, 0.07673316448926926),\n",
       "  (222000, 0.3040672838687897),\n",
       "  (223000, 0.18387974798679352),\n",
       "  (224000, 0.20383325219154358),\n",
       "  (225000, 0.20984689891338348),\n",
       "  (226000, 0.20491251349449158),\n",
       "  (227000, 0.00011888452718267217),\n",
       "  (228000, 0.18828743696212769),\n",
       "  (229000, 0.19389693439006805),\n",
       "  (230000, 0.2907979190349579),\n",
       "  (231000, 0.22258874773979187),\n",
       "  (232000, 0.2966841459274292),\n",
       "  (233000, 0.1302884966135025),\n",
       "  (234000, 0.28762516379356384),\n",
       "  (235000, 0.2320990115404129),\n",
       "  (236000, 0.2051461786031723),\n",
       "  (237000, 0.18152658641338348),\n",
       "  (238000, 0.20045410096645355),\n",
       "  (239000, 0.20162704586982727),\n",
       "  (240000, 0.0001633698120713234),\n",
       "  (241000, 0.26839396357536316),\n",
       "  (242000, 0.1868046373128891),\n",
       "  (243000, 0.3422592878341675),\n",
       "  (244000, 0.23281230032444),\n",
       "  (245000, 0.2846580743789673),\n",
       "  (246000, 0.2020072191953659),\n",
       "  (247000, 0.3047211468219757),\n",
       "  (248000, 0.20275777578353882),\n",
       "  (249000, 0.18994778394699097)],\n",
       " 'bible': [(0, 0.2497856169939041),\n",
       "  (1000, 0.24721525609493256),\n",
       "  (2000, 0.23643669486045837),\n",
       "  (3000, 0.25573065876960754),\n",
       "  (4000, 0.2564864754676819),\n",
       "  (5000, 0.26956576108932495),\n",
       "  (6000, 0.2736494541168213),\n",
       "  (7000, 0.21736405789852142),\n",
       "  (8000, 0.29998645186424255),\n",
       "  (9000, 0.23716172575950623),\n",
       "  (10000, 0.1738279014825821),\n",
       "  (11000, 0.3043056130409241),\n",
       "  (12000, 0.07531508058309555),\n",
       "  (13000, 0.41526609659194946),\n",
       "  (14000, 0.11638853698968887),\n",
       "  (15000, 0.26964467763900757),\n",
       "  (16000, 0.11312323808670044),\n",
       "  (17000, 0.131582111120224),\n",
       "  (18000, 0.0),\n",
       "  (19000, 0.008011546917259693),\n",
       "  (20000, 0.11139557510614395),\n",
       "  (21000, 0.12406782805919647),\n",
       "  (22000, 0.014561692252755165),\n",
       "  (23000, 0.0),\n",
       "  (24000, 0.1121792271733284),\n",
       "  (25000, 0.0566897951066494),\n",
       "  (26000, 0.5973389148712158),\n",
       "  (27000, 0.0811782032251358),\n",
       "  (28000, 0.0511210598051548),\n",
       "  (29000, 0.0),\n",
       "  (30000, 0.0),\n",
       "  (31000, 1.2789769243681803e-13),\n",
       "  (32000, 0.039574481546878815),\n",
       "  (33000, 0.08378918468952179),\n",
       "  (34000, 0.06367310881614685),\n",
       "  (35000, 0.028760353103280067),\n",
       "  (36000, 0.0),\n",
       "  (37000, 0.578173816204071),\n",
       "  (38000, 0.0),\n",
       "  (39000, 0.07137592881917953),\n",
       "  (40000, 0.003909730352461338),\n",
       "  (41000, 0.05617513880133629),\n",
       "  (42000, 0.0),\n",
       "  (43000, 0.12408220767974854),\n",
       "  (44000, 0.028750235214829445),\n",
       "  (45000, 0.0649326965212822),\n",
       "  (46000, 0.19692400097846985),\n",
       "  (47000, 0.0012742821127176285),\n",
       "  (48000, 0.06812132894992828),\n",
       "  (49000, 0.10374891757965088),\n",
       "  (50000, 0.03318728879094124),\n",
       "  (51000, 0.9997823238372803),\n",
       "  (52000, 0.06452745199203491),\n",
       "  (53000, 0.6535958051681519),\n",
       "  (54000, 0.031592585146427155),\n",
       "  (55000, 0.0),\n",
       "  (56000, 0.0011946620652452111),\n",
       "  (57000, 0.01250440627336502),\n",
       "  (58000, 3.946766469198337e-09),\n",
       "  (59000, 0.0),\n",
       "  (60000, 0.013524497859179974),\n",
       "  (61000, 0.0040199169889092445),\n",
       "  (62000, 0.029527541249990463),\n",
       "  (63000, 0.9999990463256836),\n",
       "  (64000, 0.08605370670557022),\n",
       "  (65000, 0.9732733368873596),\n",
       "  (66000, 0.03539837896823883),\n",
       "  (67000, 0.000808042474091053),\n",
       "  (68000, 0.001827298547141254),\n",
       "  (69000, 0.057554323226213455),\n",
       "  (70000, 0.0350503996014595),\n",
       "  (71000, 0.032210823148489),\n",
       "  (72000, 0.020833216607570648),\n",
       "  (73000, 0.0),\n",
       "  (74000, 1.2789769243681803e-13),\n",
       "  (75000, 0.0),\n",
       "  (76000, 0.0),\n",
       "  (77000, 0.17905758321285248),\n",
       "  (78000, 0.034660983830690384),\n",
       "  (79000, 0.49989452958106995),\n",
       "  (80000, 0.0),\n",
       "  (81000, 0.06641466170549393),\n",
       "  (82000, 0.0017184256576001644),\n",
       "  (83000, 3.5254040540166898e-06),\n",
       "  (84000, 0.025302132591605186),\n",
       "  (85000, 1.0903846714427345e-09),\n",
       "  (86000, 0.06775254011154175),\n",
       "  (87000, 0.04405128210783005),\n",
       "  (88000, 0.0002968227199744433),\n",
       "  (89000, 0.007345415651798248),\n",
       "  (90000, 1.1201420420547947e-05),\n",
       "  (91000, 0.00047083376557566226),\n",
       "  (92000, 0.03754507005214691),\n",
       "  (93000, 0.0),\n",
       "  (94000, 0.17998258769512177),\n",
       "  (95000, 0.06684025377035141),\n",
       "  (96000, 3.1391778065881226e-11),\n",
       "  (97000, 0.007545007858425379),\n",
       "  (98000, 0.13604147732257843),\n",
       "  (99000, 0.9910574555397034),\n",
       "  (100000, 0.9999954700469971),\n",
       "  (101000, 0.00010878479952225462),\n",
       "  (102000, 0.019020330160856247),\n",
       "  (103000, 2.0978419001949078e-08),\n",
       "  (104000, 0.0),\n",
       "  (105000, 0.005916341673582792),\n",
       "  (106000, 0.6831101179122925),\n",
       "  (107000, 0.0014083617134019732),\n",
       "  (108000, 0.0),\n",
       "  (109000, 0.05833900719881058),\n",
       "  (110000, 0.6388581395149231),\n",
       "  (111000, 0.9942374229431152),\n",
       "  (112000, 0.5946555137634277),\n",
       "  (113000, 5.684341886080802e-14),\n",
       "  (114000, 2.1614710021822248e-11),\n",
       "  (115000, 0.6505829691886902),\n",
       "  (116000, 0.7310965657234192),\n",
       "  (117000, 0.10664703696966171),\n",
       "  (118000, 0.5404244661331177),\n",
       "  (119000, 0.3316621482372284),\n",
       "  (120000, 5.581920959230047e-06),\n",
       "  (121000, 0.018895655870437622),\n",
       "  (122000, 0.8141278624534607),\n",
       "  (123000, 0.00020353157015051693),\n",
       "  (124000, 0.025994006544351578),\n",
       "  (125000, 0.0),\n",
       "  (126000, 0.0),\n",
       "  (127000, 0.00880223698914051),\n",
       "  (128000, 0.08222051709890366),\n",
       "  (129000, 0.0),\n",
       "  (130000, 0.5315848588943481),\n",
       "  (131000, 0.02067815512418747),\n",
       "  (132000, 0.06274285912513733),\n",
       "  (133000, 0.0),\n",
       "  (134000, 0.5668225288391113),\n",
       "  (135000, 0.0221774410456419),\n",
       "  (136000, 0.23981349170207977),\n",
       "  (137000, 0.036117713898420334),\n",
       "  (138000, 0.0),\n",
       "  (139000, 0.023871196433901787),\n",
       "  (140000, 0.0),\n",
       "  (141000, 0.4793160557746887),\n",
       "  (142000, 0.0),\n",
       "  (143000, 0.04528152197599411),\n",
       "  (144000, 0.0031953989528119564),\n",
       "  (145000, 0.0667807087302208),\n",
       "  (146000, 0.05915386602282524),\n",
       "  (147000, 0.35575392842292786),\n",
       "  (148000, 0.007835629396140575),\n",
       "  (149000, 9.606537787476555e-12),\n",
       "  (150000, 0.4567880928516388),\n",
       "  (151000, 0.07316019386053085),\n",
       "  (152000, 0.02630607597529888),\n",
       "  (153000, 3.6817624504692503e-09),\n",
       "  (154000, 0.023492878302931786),\n",
       "  (155000, 0.3875305950641632),\n",
       "  (156000, 0.0047097960487008095),\n",
       "  (157000, 0.556465744972229),\n",
       "  (158000, 0.0),\n",
       "  (159000, 0.0),\n",
       "  (160000, 0.5660250186920166),\n",
       "  (161000, 0.058102287352085114),\n",
       "  (162000, 0.03149138391017914),\n",
       "  (163000, 0.5231440663337708),\n",
       "  (164000, 1.4410503013095877e-08),\n",
       "  (165000, 0.07425519824028015),\n",
       "  (166000, 0.08921533823013306),\n",
       "  (167000, 0.03307546675205231),\n",
       "  (168000, 0.0),\n",
       "  (169000, 0.0006369804032146931),\n",
       "  (170000, 0.05972515791654587),\n",
       "  (171000, 0.04485582932829857),\n",
       "  (172000, 0.9778214693069458),\n",
       "  (173000, 0.02603187970817089),\n",
       "  (174000, 0.07239095121622086),\n",
       "  (175000, 0.0),\n",
       "  (176000, 0.0),\n",
       "  (177000, 0.0),\n",
       "  (178000, 0.9214009642601013),\n",
       "  (179000, 0.3630819022655487),\n",
       "  (180000, 0.0),\n",
       "  (181000, 3.573240519472165e-06),\n",
       "  (182000, 0.0),\n",
       "  (183000, 0.006551690399646759),\n",
       "  (184000, 0.008465860038995743),\n",
       "  (185000, 0.027820099145174026),\n",
       "  (186000, 0.660249650478363),\n",
       "  (187000, 1.5475620784854982e-11),\n",
       "  (188000, 0.016302019357681274),\n",
       "  (189000, 0.6131075620651245),\n",
       "  (190000, 0.02931174822151661),\n",
       "  (191000, 2.8860173983957793e-07),\n",
       "  (192000, 0.0),\n",
       "  (193000, 0.0),\n",
       "  (194000, 0.0),\n",
       "  (195000, 0.7193054556846619),\n",
       "  (196000, 0.004185936879366636),\n",
       "  (197000, 0.03863450139760971),\n",
       "  (198000, 1.134918079515046e-07),\n",
       "  (199000, 0.0),\n",
       "  (200000, 0.04107983037829399),\n",
       "  (201000, 0.00028394756373018026),\n",
       "  (202000, 0.7871902585029602),\n",
       "  (203000, 0.0),\n",
       "  (204000, 0.0),\n",
       "  (205000, 0.01769675873219967),\n",
       "  (206000, 0.9647392630577087),\n",
       "  (207000, 0.3406188488006592),\n",
       "  (208000, 6.37925268165418e-11),\n",
       "  (209000, 0.9506020545959473),\n",
       "  (210000, 8.881784197001252e-12),\n",
       "  (211000, 0.0064812153577804565),\n",
       "  (212000, 0.05279742553830147),\n",
       "  (213000, 0.08194117248058319),\n",
       "  (214000, 9.094947017729282e-13),\n",
       "  (215000, 0.04469418153166771),\n",
       "  (216000, 0.1667904555797577),\n",
       "  (217000, 0.028587881475687027),\n",
       "  (218000, 0.08621065318584442),\n",
       "  (219000, 0.10477863252162933),\n",
       "  (220000, 0.015301959589123726),\n",
       "  (221000, 0.028472375124692917),\n",
       "  (222000, 0.7810028195381165),\n",
       "  (223000, 1.68839164871315e-10),\n",
       "  (224000, 0.0198502354323864),\n",
       "  (225000, 0.022747796028852463),\n",
       "  (226000, 0.022837018594145775),\n",
       "  (227000, 0.005448041949421167),\n",
       "  (228000, 0.022568346932530403),\n",
       "  (229000, 1.176800878965878e-10),\n",
       "  (230000, 0.0017836415208876133),\n",
       "  (231000, 0.0),\n",
       "  (232000, 0.019982758909463882),\n",
       "  (233000, 4.7204703150782734e-05),\n",
       "  (234000, 0.3625143766403198),\n",
       "  (235000, 0.03307630121707916),\n",
       "  (236000, 0.004546904005110264),\n",
       "  (237000, 0.003140747547149658),\n",
       "  (238000, 0.04278968274593353),\n",
       "  (239000, 0.0134148970246315),\n",
       "  (240000, 2.4016344468691386e-12),\n",
       "  (241000, 0.0022146101109683514),\n",
       "  (242000, 0.009837165474891663),\n",
       "  (243000, 2.7853275241795927e-12),\n",
       "  (244000, 0.08234433084726334),\n",
       "  (245000, 8.185452315956354e-12),\n",
       "  (246000, 0.07392340153455734),\n",
       "  (247000, 0.5091596841812134),\n",
       "  (248000, 0.0),\n",
       "  (249000, 0.0)],\n",
       " 'tanakh': [(0, 0.24872258305549622),\n",
       "  (1000, 0.23844502866268158),\n",
       "  (2000, 0.2441060096025467),\n",
       "  (3000, 0.24100995063781738),\n",
       "  (4000, 0.24062496423721313),\n",
       "  (5000, 0.24936150014400482),\n",
       "  (6000, 0.2466319501399994),\n",
       "  (7000, 0.25130078196525574),\n",
       "  (8000, 0.23519419133663177),\n",
       "  (9000, 0.24238820374011993),\n",
       "  (10000, 0.2606649696826935),\n",
       "  (11000, 0.24435916543006897),\n",
       "  (12000, 0.24872031807899475),\n",
       "  (13000, 0.24735382199287415),\n",
       "  (14000, 0.24655583500862122),\n",
       "  (15000, 0.21827882528305054),\n",
       "  (16000, 0.26841893792152405),\n",
       "  (17000, 0.26090988516807556),\n",
       "  (18000, 0.23142556846141815),\n",
       "  (19000, 0.24977387487888336),\n",
       "  (20000, 0.2601797580718994),\n",
       "  (21000, 0.23067401349544525),\n",
       "  (22000, 0.21722754836082458),\n",
       "  (23000, 0.21048080921173096),\n",
       "  (24000, 0.009529619477689266),\n",
       "  (25000, 0.4210582673549652),\n",
       "  (26000, 0.2930457592010498),\n",
       "  (27000, 0.23995545506477356),\n",
       "  (28000, 0.2591921091079712),\n",
       "  (29000, 0.2546694576740265),\n",
       "  (30000, 0.23428182303905487),\n",
       "  (31000, 0.2753921449184418),\n",
       "  (32000, 0.2335166484117508),\n",
       "  (33000, 0.2273130565881729),\n",
       "  (34000, 0.31671398878097534),\n",
       "  (35000, 0.22163407504558563),\n",
       "  (36000, 0.004413226153701544),\n",
       "  (37000, 0.20670254528522491),\n",
       "  (38000, 0.15372149646282196),\n",
       "  (39000, 0.20670194923877716),\n",
       "  (40000, 0.28669607639312744),\n",
       "  (41000, 0.005061381962150335),\n",
       "  (42000, 0.22179104387760162),\n",
       "  (43000, 0.29597073793411255),\n",
       "  (44000, 0.21935665607452393),\n",
       "  (45000, 0.3269052803516388),\n",
       "  (46000, 0.20429839193820953),\n",
       "  (47000, 0.289873331785202),\n",
       "  (48000, 0.0016078362241387367),\n",
       "  (49000, 0.2197781652212143),\n",
       "  (50000, 0.09095866233110428),\n",
       "  (51000, 0.24071794748306274),\n",
       "  (52000, 0.06935504823923111),\n",
       "  (53000, 0.27425870299339294),\n",
       "  (54000, 0.5680095553398132),\n",
       "  (55000, 0.18703080713748932),\n",
       "  (56000, 0.0017076351214200258),\n",
       "  (57000, 0.0002520623675081879),\n",
       "  (58000, 0.0001855503796832636),\n",
       "  (59000, 0.20561276376247406),\n",
       "  (60000, 0.2086208164691925),\n",
       "  (61000, 0.0013700500130653381),\n",
       "  (62000, 0.20815379917621613),\n",
       "  (63000, 0.4890022277832031),\n",
       "  (64000, 0.21145915985107422),\n",
       "  (65000, 0.2907066345214844),\n",
       "  (66000, 0.2108003944158554),\n",
       "  (67000, 0.19134289026260376),\n",
       "  (68000, 0.2958211898803711),\n",
       "  (69000, 0.2053908258676529),\n",
       "  (70000, 0.1871500462293625),\n",
       "  (71000, 0.30845752358436584),\n",
       "  (72000, 0.1626417487859726),\n",
       "  (73000, 0.20396623015403748),\n",
       "  (74000, 0.18451914191246033),\n",
       "  (75000, 0.3441367745399475),\n",
       "  (76000, 0.40402352809906006),\n",
       "  (77000, 0.19456778466701508),\n",
       "  (78000, 0.20059195160865784),\n",
       "  (79000, 0.14736053347587585),\n",
       "  (80000, 0.08321158587932587),\n",
       "  (81000, 0.0011436777422204614),\n",
       "  (82000, 0.32717475295066833),\n",
       "  (83000, 0.19742533564567566),\n",
       "  (84000, 0.2597251236438751),\n",
       "  (85000, 0.21598435938358307),\n",
       "  (86000, 0.3088187277317047),\n",
       "  (87000, 0.21553935110569),\n",
       "  (88000, 0.20348267257213593),\n",
       "  (89000, 0.29523465037345886),\n",
       "  (90000, 0.1982620507478714),\n",
       "  (91000, 0.38789528608322144),\n",
       "  (92000, 0.006965278647840023),\n",
       "  (93000, 0.3275463581085205),\n",
       "  (94000, 0.34267011284828186),\n",
       "  (95000, 0.2163422554731369),\n",
       "  (96000, 0.2936214208602905),\n",
       "  (97000, 0.20034587383270264),\n",
       "  (98000, 0.77180016040802),\n",
       "  (99000, 0.19817809760570526),\n",
       "  (100000, 0.0857880488038063),\n",
       "  (101000, 0.2129957526922226),\n",
       "  (102000, 0.31877827644348145),\n",
       "  (103000, 0.20998980104923248),\n",
       "  (104000, 0.2927480936050415),\n",
       "  (105000, 0.2932382822036743),\n",
       "  (106000, 0.19383610785007477),\n",
       "  (107000, 0.301794171333313),\n",
       "  (108000, 0.2777964174747467),\n",
       "  (109000, 0.17331568896770477),\n",
       "  (110000, 0.16246193647384644),\n",
       "  (111000, 0.21096399426460266),\n",
       "  (112000, 0.3270508646965027),\n",
       "  (113000, 0.23049907386302948),\n",
       "  (114000, 0.17942672967910767),\n",
       "  (115000, 0.20313289761543274),\n",
       "  (116000, 0.3049037456512451),\n",
       "  (117000, 0.19930732250213623),\n",
       "  (118000, 0.19176293909549713),\n",
       "  (119000, 0.18825888633728027),\n",
       "  (120000, 0.1868213266134262),\n",
       "  (121000, 0.3743876516819),\n",
       "  (122000, 0.42194071412086487),\n",
       "  (123000, 0.19516843557357788),\n",
       "  (124000, 0.1944984346628189),\n",
       "  (125000, 0.1955917924642563),\n",
       "  (126000, 0.1920657604932785),\n",
       "  (127000, 0.13451054692268372),\n",
       "  (128000, 0.19770723581314087),\n",
       "  (129000, 0.932990312576294),\n",
       "  (130000, 0.30439668893814087),\n",
       "  (131000, 0.34430280327796936),\n",
       "  (132000, 0.17161260545253754),\n",
       "  (133000, 0.32542023062705994),\n",
       "  (134000, 0.37671494483947754),\n",
       "  (135000, 0.0014245094498619437),\n",
       "  (136000, 0.2656266391277313),\n",
       "  (137000, 0.4890855848789215),\n",
       "  (138000, 0.18713214993476868),\n",
       "  (139000, 0.43215519189834595),\n",
       "  (140000, 0.18754364550113678),\n",
       "  (141000, 0.1879781037569046),\n",
       "  (142000, 0.2901569604873657),\n",
       "  (143000, 0.07924462109804153),\n",
       "  (144000, 0.2414710372686386),\n",
       "  (145000, 0.08991428464651108),\n",
       "  (146000, 0.21008893847465515),\n",
       "  (147000, 0.0003448960487730801),\n",
       "  (148000, 0.17486238479614258),\n",
       "  (149000, 0.7268361449241638),\n",
       "  (150000, 0.012133858166635036),\n",
       "  (151000, 0.3485441207885742),\n",
       "  (152000, 0.005162105895578861),\n",
       "  (153000, 0.16006796061992645),\n",
       "  (154000, 0.18723726272583008),\n",
       "  (155000, 0.2938670516014099),\n",
       "  (156000, 0.3328116834163666),\n",
       "  (157000, 0.17934730648994446),\n",
       "  (158000, 0.3709670603275299),\n",
       "  (159000, 0.3158278167247772),\n",
       "  (160000, 0.18378907442092896),\n",
       "  (161000, 0.31068459153175354),\n",
       "  (162000, 0.18287359178066254),\n",
       "  (163000, 0.0029996149241924286),\n",
       "  (164000, 0.2768687605857849),\n",
       "  (165000, 0.17805078625679016),\n",
       "  (166000, 0.12692934274673462),\n",
       "  (167000, 0.20568028092384338),\n",
       "  (168000, 0.18764519691467285),\n",
       "  (169000, 0.1863822191953659),\n",
       "  (170000, 0.4398556053638458),\n",
       "  (171000, 0.024240994825959206),\n",
       "  (172000, 0.17238183319568634),\n",
       "  (173000, 0.17315968871116638),\n",
       "  (174000, 0.3325119912624359),\n",
       "  (175000, 0.7382962107658386),\n",
       "  (176000, 0.37351325154304504),\n",
       "  (177000, 0.6250844597816467),\n",
       "  (178000, 0.17839936912059784),\n",
       "  (179000, 0.3618015646934509),\n",
       "  (180000, 0.3489223122596741),\n",
       "  (181000, 0.17062832415103912),\n",
       "  (182000, 0.17149047553539276),\n",
       "  (183000, 0.3593437969684601),\n",
       "  (184000, 0.18694821000099182),\n",
       "  (185000, 0.15190014243125916),\n",
       "  (186000, 0.3355834186077118),\n",
       "  (187000, 0.20204691588878632),\n",
       "  (188000, 0.27208012342453003),\n",
       "  (189000, 0.2277446985244751),\n",
       "  (190000, 0.1738990694284439),\n",
       "  (191000, 0.09052173793315887),\n",
       "  (192000, 0.003389974357560277),\n",
       "  (193000, 0.3908768594264984),\n",
       "  (194000, 0.378824383020401),\n",
       "  (195000, 0.25872331857681274),\n",
       "  (196000, 0.1715720146894455),\n",
       "  (197000, 0.1947687864303589),\n",
       "  (198000, 1.0),\n",
       "  (199000, 0.3949521481990814),\n",
       "  (200000, 0.18008412420749664),\n",
       "  (201000, 0.1714266538619995),\n",
       "  (202000, 0.17962141335010529),\n",
       "  (203000, 0.22572794556617737),\n",
       "  (204000, 0.17730334401130676),\n",
       "  (205000, 0.3424229919910431),\n",
       "  (206000, 0.2245875597000122),\n",
       "  (207000, 0.17282114923000336),\n",
       "  (208000, 0.3991954028606415),\n",
       "  (209000, 0.34109678864479065),\n",
       "  (210000, 0.972896933555603),\n",
       "  (211000, 0.33238503336906433),\n",
       "  (212000, 0.20533788204193115),\n",
       "  (213000, 0.017078233882784843),\n",
       "  (214000, 0.15598013997077942),\n",
       "  (215000, 0.35185104608535767),\n",
       "  (216000, 0.1543450802564621),\n",
       "  (217000, 0.3445774018764496),\n",
       "  (218000, 0.31687602400779724),\n",
       "  (219000, 0.012505387887358665),\n",
       "  (220000, 0.1707008183002472),\n",
       "  (221000, 0.29073235392570496),\n",
       "  (222000, 0.5293721556663513),\n",
       "  (223000, 0.01902063377201557),\n",
       "  (224000, 0.33824989199638367),\n",
       "  (225000, 0.605683445930481),\n",
       "  (226000, 0.3245197832584381),\n",
       "  (227000, 1.0),\n",
       "  (228000, 0.2886030375957489),\n",
       "  (229000, 0.1982896625995636),\n",
       "  (230000, 0.22691069543361664),\n",
       "  (231000, 0.17779912054538727),\n",
       "  (232000, 0.2794678509235382),\n",
       "  (233000, 0.3433561325073242),\n",
       "  (234000, 0.0009520641178824008),\n",
       "  (235000, 0.18299055099487305),\n",
       "  (236000, 0.19149711728096008),\n",
       "  (237000, 0.31587672233581543),\n",
       "  (238000, 0.2833966016769409),\n",
       "  (239000, 0.19785988330841064),\n",
       "  (240000, 0.0022781670559197664),\n",
       "  (241000, 0.1782657504081726),\n",
       "  (242000, 0.07108109444379807),\n",
       "  (243000, 0.1724131554365158),\n",
       "  (244000, 0.37223950028419495),\n",
       "  (245000, 0.20350518822669983),\n",
       "  (246000, 0.26803719997406006),\n",
       "  (247000, 0.020655682310461998),\n",
       "  (248000, 0.33953234553337097),\n",
       "  (249000, 0.17251235246658325)],\n",
       " 'quran': [(0, 0.25243324041366577),\n",
       "  (1000, 0.24677065014839172),\n",
       "  (2000, 0.24219286441802979),\n",
       "  (3000, 0.24709264934062958),\n",
       "  (4000, 0.2373056411743164),\n",
       "  (5000, 0.2351531833410263),\n",
       "  (6000, 0.24241654574871063),\n",
       "  (7000, 0.22974948585033417),\n",
       "  (8000, 0.24444930255413055),\n",
       "  (9000, 0.23712977766990662),\n",
       "  (10000, 0.25728148221969604),\n",
       "  (11000, 0.25772929191589355),\n",
       "  (12000, 0.18605613708496094),\n",
       "  (13000, 0.24840690195560455),\n",
       "  (14000, 0.25498345494270325),\n",
       "  (15000, 0.24232202768325806),\n",
       "  (16000, 0.15729932487010956),\n",
       "  (17000, 0.3301025331020355),\n",
       "  (18000, 0.24706728756427765),\n",
       "  (19000, 0.23765262961387634),\n",
       "  (20000, 0.2576124966144562),\n",
       "  (21000, 0.17349809408187866),\n",
       "  (22000, 0.2487015277147293),\n",
       "  (23000, 0.24955739080905914),\n",
       "  (24000, 0.2365877479314804),\n",
       "  (25000, 0.2591663897037506),\n",
       "  (26000, 0.24303527176380157),\n",
       "  (27000, 0.40158945322036743),\n",
       "  (28000, 0.23150059580802917),\n",
       "  (29000, 0.28834864497184753),\n",
       "  (30000, 0.28219807147979736),\n",
       "  (31000, 0.22261551022529602),\n",
       "  (32000, 0.26927047967910767),\n",
       "  (33000, 0.1782989203929901),\n",
       "  (34000, 0.24167756736278534),\n",
       "  (35000, 0.22293977439403534),\n",
       "  (36000, 0.3049260973930359),\n",
       "  (37000, 0.23315086960792542),\n",
       "  (38000, 0.28132155537605286),\n",
       "  (39000, 0.2432987540960312),\n",
       "  (40000, 0.20561479032039642),\n",
       "  (41000, 0.2657381594181061),\n",
       "  (42000, 0.22017891705036163),\n",
       "  (43000, 0.19405660033226013),\n",
       "  (44000, 0.290568083524704),\n",
       "  (45000, 0.21859228610992432),\n",
       "  (46000, 0.21615734696388245),\n",
       "  (47000, 0.2598331570625305),\n",
       "  (48000, 0.22067224979400635),\n",
       "  (49000, 0.2608264684677124),\n",
       "  (50000, 0.22634999454021454),\n",
       "  (51000, 0.2335648387670517),\n",
       "  (52000, 0.25889676809310913),\n",
       "  (53000, 0.20549169182777405),\n",
       "  (54000, 0.1812354177236557),\n",
       "  (55000, 0.0859498605132103),\n",
       "  (56000, 0.019979560747742653),\n",
       "  (57000, 0.29618576169013977),\n",
       "  (58000, 0.20979054272174835),\n",
       "  (59000, 0.21549317240715027),\n",
       "  (60000, 0.19132375717163086),\n",
       "  (61000, 0.19971109926700592),\n",
       "  (62000, 0.013135387562215328),\n",
       "  (63000, 0.14002254605293274),\n",
       "  (64000, 0.18111465871334076),\n",
       "  (65000, 0.17170360684394836),\n",
       "  (66000, 0.18544058501720428),\n",
       "  (67000, 0.2891986668109894),\n",
       "  (68000, 0.2906952500343323),\n",
       "  (69000, 0.20629620552062988),\n",
       "  (70000, 0.20423109829425812),\n",
       "  (71000, 0.215873122215271),\n",
       "  (72000, 0.18859462440013885),\n",
       "  (73000, 0.21772480010986328),\n",
       "  (74000, 0.24478943645954132),\n",
       "  (75000, 0.20043092966079712),\n",
       "  (76000, 0.21536695957183838),\n",
       "  (77000, 0.21774518489837646),\n",
       "  (78000, 0.1967528611421585),\n",
       "  (79000, 0.1841435432434082),\n",
       "  (80000, 0.2698565423488617),\n",
       "  (81000, 0.3379824757575989),\n",
       "  (82000, 0.21862588822841644),\n",
       "  (83000, 0.212899312376976),\n",
       "  (84000, 0.3056672513484955),\n",
       "  (85000, 0.21506716310977936),\n",
       "  (86000, 0.29542991518974304),\n",
       "  (87000, 0.00022999443172011524),\n",
       "  (88000, 0.2268294394016266),\n",
       "  (89000, 0.29997846484184265),\n",
       "  (90000, 0.049558237195014954),\n",
       "  (91000, 0.19197656214237213),\n",
       "  (92000, 0.29422006011009216),\n",
       "  (93000, 0.2512660026550293),\n",
       "  (94000, 0.2136913686990738),\n",
       "  (95000, 0.7975674867630005),\n",
       "  (96000, 0.28914085030555725),\n",
       "  (97000, 0.42095351219177246),\n",
       "  (98000, 0.2346857190132141),\n",
       "  (99000, 0.20469754934310913),\n",
       "  (100000, 0.1920321136713028),\n",
       "  (101000, 0.29241859912872314),\n",
       "  (102000, 0.32240554690361023),\n",
       "  (103000, 0.30738404393196106),\n",
       "  (104000, 0.19331112504005432),\n",
       "  (105000, 0.2378503978252411),\n",
       "  (106000, 0.007046038750559092),\n",
       "  (107000, 0.15554215013980865),\n",
       "  (108000, 0.19479748606681824),\n",
       "  (109000, 0.1954631805419922),\n",
       "  (110000, 0.15794120728969574),\n",
       "  (111000, 0.192109614610672),\n",
       "  (112000, 0.29597488045692444),\n",
       "  (113000, 0.16306732594966888),\n",
       "  (114000, 0.7170005440711975),\n",
       "  (115000, 0.004981225822120905),\n",
       "  (116000, 0.19519732892513275),\n",
       "  (117000, 0.2719120681285858),\n",
       "  (118000, 0.9292888641357422),\n",
       "  (119000, 0.20257407426834106),\n",
       "  (120000, 0.27282461524009705),\n",
       "  (121000, 0.9041283130645752),\n",
       "  (122000, 0.2280428409576416),\n",
       "  (123000, 0.18977011740207672),\n",
       "  (124000, 0.2021607756614685),\n",
       "  (125000, 0.1939283311367035),\n",
       "  (126000, 0.21298947930335999),\n",
       "  (127000, 0.3433065414428711),\n",
       "  (128000, 0.1907181292772293),\n",
       "  (129000, 0.390927255153656),\n",
       "  (130000, 0.20123286545276642),\n",
       "  (131000, 0.21497796475887299),\n",
       "  (132000, 0.19430463016033173),\n",
       "  (133000, 0.16828611493110657),\n",
       "  (134000, 0.00575080094859004),\n",
       "  (135000, 0.4133062958717346),\n",
       "  (136000, 0.19350197911262512),\n",
       "  (137000, 0.30259785056114197),\n",
       "  (138000, 0.19737356901168823),\n",
       "  (139000, 0.055619534105062485),\n",
       "  (140000, 0.29672572016716003),\n",
       "  (141000, 0.36136946082115173),\n",
       "  (142000, 0.00012514401169028133),\n",
       "  (143000, 0.21139639616012573),\n",
       "  (144000, 0.4958859384059906),\n",
       "  (145000, 0.16683213412761688),\n",
       "  (146000, 0.23881667852401733),\n",
       "  (147000, 0.049898236989974976),\n",
       "  (148000, 0.0013216614024713635),\n",
       "  (149000, 0.20954249799251556),\n",
       "  (150000, 0.11522367596626282),\n",
       "  (151000, 0.13984520733356476),\n",
       "  (152000, 0.1992647498846054),\n",
       "  (153000, 0.0029533542692661285),\n",
       "  (154000, 0.19751709699630737),\n",
       "  (155000, 0.011907150968909264),\n",
       "  (156000, 0.21483276784420013),\n",
       "  (157000, 0.2146068662405014),\n",
       "  (158000, 0.1834714412689209),\n",
       "  (159000, 0.37339508533477783),\n",
       "  (160000, 0.18746301531791687),\n",
       "  (161000, 0.28657597303390503),\n",
       "  (162000, 0.3362710475921631),\n",
       "  (163000, 0.054807014763355255),\n",
       "  (164000, 0.16309674084186554),\n",
       "  (165000, 0.32263094186782837),\n",
       "  (166000, 0.17568039894104004),\n",
       "  (167000, 0.2823197841644287),\n",
       "  (168000, 0.010460300371050835),\n",
       "  (169000, 0.7485347986221313),\n",
       "  (170000, 0.3925148844718933),\n",
       "  (171000, 0.14849169552326202),\n",
       "  (172000, 0.1756887137889862),\n",
       "  (173000, 0.007081767078489065),\n",
       "  (174000, 0.27268218994140625),\n",
       "  (175000, 0.1630343347787857),\n",
       "  (176000, 0.3753717541694641),\n",
       "  (177000, 0.9999821186065674),\n",
       "  (178000, 0.2608124017715454),\n",
       "  (179000, 0.20773644745349884),\n",
       "  (180000, 0.19971665740013123),\n",
       "  (181000, 0.16722092032432556),\n",
       "  (182000, 0.16705560684204102),\n",
       "  (183000, 0.21958540380001068),\n",
       "  (184000, 0.1572895050048828),\n",
       "  (185000, 0.30972179770469666),\n",
       "  (186000, 0.11286533623933792),\n",
       "  (187000, 0.18414196372032166),\n",
       "  (188000, 0.23921844363212585),\n",
       "  (189000, 0.20904338359832764),\n",
       "  (190000, 0.6772788763046265),\n",
       "  (191000, 0.3295660614967346),\n",
       "  (192000, 0.18794786930084229),\n",
       "  (193000, 0.30737605690956116),\n",
       "  (194000, 0.14812247455120087),\n",
       "  (195000, 0.27826082706451416),\n",
       "  (196000, 0.1611909419298172),\n",
       "  (197000, 0.46767503023147583),\n",
       "  (198000, 0.3408143222332001),\n",
       "  (199000, 0.17613020539283752),\n",
       "  (200000, 0.17941121757030487),\n",
       "  (201000, 0.26227250695228577),\n",
       "  (202000, 0.2294221818447113),\n",
       "  (203000, 0.1881365031003952),\n",
       "  (204000, 0.3176983892917633),\n",
       "  (205000, 0.333924800157547),\n",
       "  (206000, 0.12892210483551025),\n",
       "  (207000, 0.18290303647518158),\n",
       "  (208000, 0.10387547314167023),\n",
       "  (209000, 0.3700748383998871),\n",
       "  (210000, 0.24862827360630035),\n",
       "  (211000, 0.3266415596008301),\n",
       "  (212000, 0.9574178457260132),\n",
       "  (213000, 0.35054558515548706),\n",
       "  (214000, 0.389238178730011),\n",
       "  (215000, 0.34139156341552734),\n",
       "  (216000, 0.18411244451999664),\n",
       "  (217000, 0.2256077229976654),\n",
       "  (218000, 0.16119389235973358),\n",
       "  (219000, 0.1692480593919754),\n",
       "  (220000, 0.1584608405828476),\n",
       "  (221000, 0.33787277340888977),\n",
       "  (222000, 0.025276491418480873),\n",
       "  (223000, 0.9156870245933533),\n",
       "  (224000, 0.35004889965057373),\n",
       "  (225000, 0.43309271335601807),\n",
       "  (226000, 0.45542603731155396),\n",
       "  (227000, 0.17752595245838165),\n",
       "  (228000, 0.2156723141670227),\n",
       "  (229000, 0.6927266716957092),\n",
       "  (230000, 0.11888693273067474),\n",
       "  (231000, 0.2162187099456787),\n",
       "  (232000, 0.00014569210179615766),\n",
       "  (233000, 0.1894591599702835),\n",
       "  (234000, 0.8479519486427307),\n",
       "  (235000, 0.3034467399120331),\n",
       "  (236000, 0.3597915470600128),\n",
       "  (237000, 0.267964243888855),\n",
       "  (238000, 0.020173652097582817),\n",
       "  (239000, 0.1878577023744583),\n",
       "  (240000, 0.3021302819252014),\n",
       "  (241000, 0.9711064696311951),\n",
       "  (242000, 0.05545353889465332),\n",
       "  (243000, 0.3203411400318146),\n",
       "  (244000, 0.029262954369187355),\n",
       "  (245000, 0.30364593863487244),\n",
       "  (246000, 0.17741160094738007),\n",
       "  (247000, 0.18760396540164948),\n",
       "  (248000, 0.11456725746393204),\n",
       "  (249000, 0.09692656993865967)]}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_obj('losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(quran_decoder, 'quran_decoder')\n",
    "save_obj(sutras_decoder, 'sutras_decoder')\n",
    "save_obj(vedas_decoder, 'vedas_decoder')\n",
    "save_obj(bible_decoder, 'bible_decoder')\n",
    "save_obj(tanakh_decoder, 'tanakh_decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
